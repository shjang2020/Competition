{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IDEA**\n",
    "- 충돌의 여부를 판단함에 있어 날씨와 시간 또한 중요한 역할을 하기 때문에 충돌 여부를 판단하기 전 날씨와 시간을 먼저 예측하는 것이 좋을 것\n",
    "    - 날씨나 시간을 예측할 때는 Scene Recognition 방법을 사용하는 것도 좋을 것으로 보임\n",
    "\n",
    "\n",
    "- Crash와 Ego의 경우 묶어서 판단하는 것이 좋을 것으로 보임. 그렇기 때문에\n",
    "    - 0: 충돌안함, 1: 나랑 충돌, 2: 나랑 충돌 안함 으로 구분\n",
    "    \n",
    "- 여기서 문제는 crash가 0인 경우 ego, weather, timing에 대한 값이 없기 때문에 우선적으로는 crash와 ego를 먼저 예측해야할 것으로 보인다.\n",
    "    - 그래서 영상을 통해 crash와 ego를 예측한 다음, 값을 이용하여 weather와 timing을 예측하고, 이를 \n",
    "\n",
    "**결론**\n",
    "\n",
    "1. 날씨와 시간의 경우 영상 또는 이미지를 기반으로 먼저 분류(Classification)\n",
    "2. 영상 + 예측한 날씨, 시간 값 attention으로 활용하여 Crash+Ego의 예측\n",
    "3. 예측한 값들을 라벨링하여 결과값 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**썸네일 이미지 추출**\\\n",
    "cv2.VideoCapture() 함수는 비디오 파일 \"example.mp4\"를 열고 VideoCapture 개체를 반환합니다.\\\n",
    "read() 메서드는 비디오의 첫 번째 프레임을 읽고 성공(프레임이 성공적으로 읽혔는지 여부를 나타내는 부울)과 프레임(프레임을 나타내는 NumPy 배열)의 튜플을 반환합니다.\\\n",
    "cv2.resize() 함수는 프레임 크기를 256x256 썸네일 이미지로 조정합니다. 마지막으로 cv2.imwrite() 함수는 섬네일 이미지를 \"example_thumbnail.jpg\"라는 파일에 저장합니다.\\\n",
    "(폭, 높이)의 다른 튜플을 cv2.resize() 함수에 전달하여 축소 이미지의 크기를 조정할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2a99e1929424e3e8a2e9dea14c9ab5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2698 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(os.listdir('train')):\n",
    "    i = i.split('.mp4')[0]\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(f'train/{i}.mp4')\n",
    "\n",
    "    # Read the first frame\n",
    "    success, frame = video.read()\n",
    "\n",
    "    # Create a thumbnail image\n",
    "    thumbnail = cv2.resize(frame, (256, 256))\n",
    "\n",
    "    # Save the thumbnail image\n",
    "    cv2.imwrite(f'train_cap/{i}.jpg', thumbnail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "303c7d55ddb445de805f408ab276176e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1800 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in tqdm(os.listdir('test')):\n",
    "    i = i.split('.mp4')[0]\n",
    "    # Open the video file\n",
    "    video = cv2.VideoCapture(f'test/{i}.mp4')\n",
    "\n",
    "    # Read the first frame\n",
    "    success, frame = video.read()\n",
    "\n",
    "    # Create a thumbnail image\n",
    "    thumbnail = cv2.resize(frame, (256, 256))\n",
    "\n",
    "    # Save the thumbnail image\n",
    "    cv2.imwrite(f'test_cap/{i}.jpg', thumbnail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2698, 2698)"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('train')), len(os.listdir('train_cap'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'.ipynb_checkpoints'}"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "set(pd.DataFrame(os.listdir('train_cap'))[0].apply(lambda x: x.split('.jpg')[0]))-set(pd.DataFrame(os.listdir('train'))[0].apply(lambda x: x.split('.mp4')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1800, 1800)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(os.listdir('test')), len(os.listdir('test_cap'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
