{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 변수 설명 및 대회 전략\n",
    "+ GitHub README에 다 적어놨습니다.\n",
    "+ 중복데이터 처리를 어떻게 해야할지 고민을 하였습니다.\n",
    "+ Feature Engineering보단 Catboost의 Categorical 데이터에 관한 하이퍼파라미터 설정에 포커스를 맞췄습니다.\n",
    "+ [GitHub](https://github.com/ds-wook/PredictCreditCardDelinquency)\n",
    "\n",
    "# 참고한 자료\n",
    "+ [Hayo 님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2519?page=1&dtype=recent)\n",
    "+ [rollcake님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2526?page=1&dtype=recent)\n",
    "+ [datu님의 노트북](https://dacon.io/competitions/official/235713/codeshare/2515?page=2&dtype=recent)\n",
    "+ [catboost 논문](https://arxiv.org/pdf/1706.09516.pdf)\n",
    "+ [catboost 활용 논문](https://arxiv.org/abs/2104.07553)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:17:27.333498Z",
     "iopub.status.busy": "2021-05-24T03:17:27.333131Z",
     "iopub.status.idle": "2021-05-24T03:17:57.518299Z",
     "shell.execute_reply": "2021-05-24T03:17:57.517366Z",
     "shell.execute_reply.started": "2021-05-24T03:17:27.333420Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping typing as it is not installed.\u001b[0m\n",
      "Collecting pytorch_tabnet\n",
      "  Cloning https://github.com/dreamquark-ai/tabnet.git (to revision develop) to /tmp/pip-install-2ke2ioc2/pytorch-tabnet_05b3f8a626bb4a6994d213e123b81bcb\n",
      "  Running command git clone -q https://github.com/dreamquark-ai/tabnet.git /tmp/pip-install-2ke2ioc2/pytorch-tabnet_05b3f8a626bb4a6994d213e123b81bcb\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h    Preparing wheel metadata ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.19.5)\n",
      "Requirement already satisfied: scikit_learn>0.21 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (0.24.1)\n",
      "Requirement already satisfied: scipy>1.4 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.5.4)\n",
      "Requirement already satisfied: torch<2.0,>=1.2 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (1.7.0)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.36 in /opt/conda/lib/python3.7/site-packages (from pytorch_tabnet) (4.59.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch_tabnet) (1.0.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.7/site-packages (from scikit_learn>0.21->pytorch_tabnet) (2.1.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (0.18.2)\n",
      "Requirement already satisfied: typing_extensions in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (3.7.4.3)\n",
      "Requirement already satisfied: dataclasses in /opt/conda/lib/python3.7/site-packages (from torch<2.0,>=1.2->pytorch_tabnet) (0.6)\n",
      "Building wheels for collected packages: pytorch-tabnet\n",
      "  Building wheel for pytorch-tabnet (PEP 517) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for pytorch-tabnet: filename=pytorch_tabnet-3.1.1-py3-none-any.whl size=39326 sha256=dbf370437df193bcfe8cb3ee932ec034b2624d8b3cea2c676b2862d1c401b1dc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-xa0gkqiu/wheels/a6/8e/aa/6f5ef6a2e389c8b5f7ea1c74bbb03ece8773b03c2b8955c334\n",
      "Successfully built pytorch-tabnet\n",
      "Installing collected packages: pytorch-tabnet\n",
      "Successfully installed pytorch-tabnet-3.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall -y typing # this should avoid  AttributeError: type object 'Callable' has no attribute '_abc_registry'\n",
    "!pip install  \"git+https://github.com/dreamquark-ai/tabnet.git@develop#egg=pytorch_tabnet\" --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2021-05-24T03:17:57.522016Z",
     "iopub.status.busy": "2021-05-24T03:17:57.521745Z",
     "iopub.status.idle": "2021-05-24T03:18:01.889106Z",
     "shell.execute_reply": "2021-05-24T03:18:01.888260Z",
     "shell.execute_reply.started": "2021-05-24T03:17:57.521975Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-1-cc5861bb394b>\", line 8, in <module>\n",
      "    from lightgbm import LGBMClassifier\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\lightgbm\\__init__.py\", line 8, in <module>\n",
      "    from .basic import Booster, Dataset\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\", line 14, in <module>\n",
      "    from .compat import (PANDAS_INSTALLED, DataFrame, Series, is_dtype_sparse,\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\lightgbm\\compat.py\", line 114, in <module>\n",
      "    from sklearn.base import BaseEstimator\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\", line 82, in <module>\n",
      "    from .base import clone\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 17, in <module>\n",
      "    from .utils import _IS_32BIT\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\", line 23, in <module>\n",
      "    from .class_weight import compute_class_weight, compute_sample_weight\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\class_weight.py\", line 7, in <module>\n",
      "    from .validation import _deprecate_positional_args\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\", line 26, in <module>\n",
      "    from .fixes import _object_dtype_isnan, parse_version\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\", line 28, in <module>\n",
      "    from pkg_resources import parse_version  # type: ignore\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 3243, in <module>\n",
      "    def _initialize_master_working_set():\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 3226, in _call_aside\n",
      "    f(*args, **kwargs)\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 3255, in _initialize_master_working_set\n",
      "    working_set = WorkingSet._build_master()\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 559, in _build_master\n",
      "    ws = cls()\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 552, in __init__\n",
      "    self.add_entry(entry)\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 608, in add_entry\n",
      "    for dist in find_distributions(entry, True):\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 2062, in find_on_path\n",
      "    factory = dist_factory(path_item, entry, only)\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\", line 2073, in dist_factory\n",
      "    os.path.isdir(os.path.join(path_item, entry))\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\genericpath.py\", line 42, in isdir\n",
      "    st = os.stat(s)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\inspect.py\", line 1503, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\inspect.py\", line 1461, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\inspect.py\", line 708, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\inspect.py\", line 754, in getmodule\n",
      "    os.path.realpath(f)] = module.__name__\n",
      "  File \"C:\\Users\\JangSeongHyun\\anaconda3\\lib\\ntpath.py\", line 664, in realpath\n",
      "    if _getfinalpathname(spath) == path:\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'NoneType' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-cc5861bb394b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcatboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mCatBoostClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mPool\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLGBMClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbasic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBooster\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m from .callback import (early_stopping, print_evaluation, record_evaluation,\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\basic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m from .compat import (PANDAS_INSTALLED, DataFrame, Series, is_dtype_sparse,\n\u001b[0m\u001b[0;32m     15\u001b[0m                      \u001b[0mDataTable\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\lightgbm\\compat.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaseEstimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mRegressorMixin\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mClassifierMixin\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[1;31m# noqa: F401\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m     \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\base.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0m_config\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 17\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_IS_32BIT\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     18\u001b[0m from .utils._tags import (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mmurmurhash\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmurmurhash3_32\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 23\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mclass_weight\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcompute_class_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     24\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_joblib\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\class_weight.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mvalidation\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_deprecate_positional_args\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mfixes\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0m_object_dtype_isnan\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparse_version\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[1;33m.\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_config\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_get_config\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m     \u001b[1;32mfrom\u001b[0m \u001b[0mpkg_resources\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mparse_version\u001b[0m  \u001b[1;31m# type: ignore\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m \u001b[1;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m   3242\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0m_call_aside\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3243\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0m_initialize_master_working_set\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3244\u001b[0m     \"\"\"\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_call_aside\u001b[1;34m(f, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3225\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_call_aside\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3226\u001b[1;33m     \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3227\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_initialize_master_working_set\u001b[1;34m()\u001b[0m\n\u001b[0;32m   3254\u001b[0m     \"\"\"\n\u001b[1;32m-> 3255\u001b[1;33m     \u001b[0mworking_set\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWorkingSet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_build_master\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3256\u001b[0m     \u001b[0m_declare_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'object'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mworking_set\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworking_set\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m_build_master\u001b[1;34m(cls)\u001b[0m\n\u001b[0;32m    558\u001b[0m         \"\"\"\n\u001b[1;32m--> 559\u001b[1;33m         \u001b[0mws\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, entries)\u001b[0m\n\u001b[0;32m    551\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 552\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_entry\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    553\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36madd_entry\u001b[1;34m(self, entry)\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mentries\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 608\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfind_distributions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    609\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mfind_on_path\u001b[1;34m(importer, path_item, only)\u001b[0m\n\u001b[0;32m   2061\u001b[0m         \u001b[0mfullpath\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2062\u001b[1;33m         \u001b[0mfactory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdist_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0monly\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2063\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdist\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfactory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfullpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pkg_resources\\__init__.py\u001b[0m in \u001b[0;36mdist_factory\u001b[1;34m(path_item, entry, only)\u001b[0m\n\u001b[0;32m   2072\u001b[0m         \u001b[0mlower\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.dist-info'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2073\u001b[1;33m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_item\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mentry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2074\u001b[0m     )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\genericpath.py\u001b[0m in \u001b[0;36misdir\u001b[1;34m(s)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 42\u001b[1;33m         \u001b[0mst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     43\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2044\u001b[0m                         \u001b[1;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2045\u001b[1;33m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2046\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'KeyboardInterrupt' object has no attribute '_render_traceback_'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2045\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2046\u001b[0m                     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2047\u001b[1;33m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[0;32m   2048\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[0;32m   2049\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1434\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1437\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[0;32m   1438\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1334\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1335\u001b[0m             \u001b[1;31m# Verbose modes need a full traceback\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1336\u001b[1;33m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[0;32m   1337\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1338\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1191\u001b[0m         \u001b[1;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1193\u001b[1;33m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[0;32m   1194\u001b[0m                                                                tb_offset)\n\u001b[0;32m   1195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1151\u001b[1;33m         \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1152\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1153\u001b[0m         \u001b[0mframes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\IPython\\core\\ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[1;34m(etype, value, records)\u001b[0m\n\u001b[0;32m    449\u001b[0m     \u001b[1;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    450\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0metype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 451\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    452\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    453\u001b[0m     \u001b[1;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"
     ]
    }
   ],
   "source": [
    "from typing import Dict, Tuple, Union, List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from catboost import CatBoostClassifier, Pool\n",
    "from lightgbm import LGBMClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset\n",
    "+ catboost가 이 대회의 핵심 알고리즘이라고 생각하여 categorical한 변수를 만드는게 중요하다고 생각하여 따로 함수를 만듬\n",
    "+ 다른 알고리즘들은 categorical한 변수보다 numeric한 변수 처리에 초점을 맞춤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:01.891168Z",
     "iopub.status.busy": "2021-05-24T03:18:01.890877Z",
     "iopub.status.idle": "2021-05-24T03:18:01.947487Z",
     "shell.execute_reply": "2021-05-24T03:18:01.946520Z",
     "shell.execute_reply.started": "2021-05-24T03:18:01.891140Z"
    }
   },
   "outputs": [],
   "source": [
    "def category_income(data: pd.DataFrame) -> pd.DataFrame:\n",
    "    data[\"income_total\"] = data[\"income_total\"] / 10000\n",
    "    conditions = [\n",
    "        (data[\"income_total\"].le(18)),\n",
    "        (data[\"income_total\"].gt(18) & data[\"income_total\"].le(33)),\n",
    "        (data[\"income_total\"].gt(33) & data[\"income_total\"].le(49)),\n",
    "        (data[\"income_total\"].gt(49) & data[\"income_total\"].le(64)),\n",
    "        (data[\"income_total\"].gt(64) & data[\"income_total\"].le(80)),\n",
    "        (data[\"income_total\"].gt(80) & data[\"income_total\"].le(95)),\n",
    "        (data[\"income_total\"].gt(95) & data[\"income_total\"].le(111)),\n",
    "        (data[\"income_total\"].gt(111) & data[\"income_total\"].le(126)),\n",
    "        (data[\"income_total\"].gt(126) & data[\"income_total\"].le(142)),\n",
    "        (data[\"income_total\"].gt(142)),\n",
    "    ]\n",
    "    choices = [i for i in range(10)]\n",
    "\n",
    "    data[\"income_total\"] = np.select(conditions, choices)\n",
    "    return data\n",
    "\n",
    "\n",
    "def load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    path = \"../input/predictcreditcarddelinquency/\"\n",
    "    train = pd.read_csv(path + \"train.csv\")\n",
    "    train = train.drop([\"index\"], axis=1)\n",
    "    train.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "    test = pd.read_csv(path + \"test.csv\")\n",
    "    test = test.drop([\"index\"], axis=1)\n",
    "    test.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "    # absolute\n",
    "    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n",
    "    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n",
    "    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n",
    "    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n",
    "    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n",
    "    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n",
    "\n",
    "    # DAYS_BIRTH\n",
    "    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n",
    "\n",
    "    # Age\n",
    "    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n",
    "    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n",
    "\n",
    "    # DAYS_EMPLOYED\n",
    "    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "\n",
    "    # EMPLOYED\n",
    "    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n",
    "    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n",
    "\n",
    "    # before_EMPLOYED\n",
    "    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n",
    "    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n",
    "    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n",
    "    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n",
    "    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n",
    "    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n",
    "\n",
    "    # gender_car_reality\n",
    "    train[\"user_code\"] = (\n",
    "        train[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"reality\"].astype(str)\n",
    "    )\n",
    "    test[\"user_code\"] = (\n",
    "        test[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"reality\"].astype(str)\n",
    "    )\n",
    "\n",
    "    del_cols = [\n",
    "        \"gender\",\n",
    "        \"car\",\n",
    "        \"reality\",\n",
    "        \"email\",\n",
    "        \"child_num\",\n",
    "        \"DAYS_BIRTH\",\n",
    "        \"DAYS_EMPLOYED\",\n",
    "    ]\n",
    "    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n",
    "    train.drop(del_cols, axis=1, inplace=True)\n",
    "    test.drop(del_cols, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = [\n",
    "        \"income_type\",\n",
    "        \"edu_type\",\n",
    "        \"family_type\",\n",
    "        \"house_type\",\n",
    "        \"occyp_type\",\n",
    "        \"user_code\",\n",
    "    ]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(train[col])\n",
    "        train[col] = label_encoder.transform(train[col])\n",
    "        test[col] = label_encoder.transform(test[col])\n",
    "\n",
    "    return train, test\n",
    "\n",
    "\n",
    "def cat_load_dataset() -> Tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    path = \"../input/predictcreditcarddelinquency/\"\n",
    "    train = pd.read_csv(path + \"train.csv\")\n",
    "    train = train.drop([\"index\"], axis=1)\n",
    "    train.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "    test = pd.read_csv(path + \"test.csv\")\n",
    "    test = test.drop([\"index\"], axis=1)\n",
    "    test.fillna(\"NAN\", inplace=True)\n",
    "\n",
    "    # absolute\n",
    "    train[\"DAYS_EMPLOYED\"] = train[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    train[\"DAYS_EMPLOYED\"] = np.abs(train[\"DAYS_EMPLOYED\"])\n",
    "    test[\"DAYS_EMPLOYED\"] = test[\"DAYS_EMPLOYED\"].map(lambda x: 0 if x > 0 else x)\n",
    "    test[\"DAYS_EMPLOYED\"] = np.abs(test[\"DAYS_EMPLOYED\"])\n",
    "    train[\"DAYS_BIRTH\"] = np.abs(train[\"DAYS_BIRTH\"])\n",
    "    test[\"DAYS_BIRTH\"] = np.abs(test[\"DAYS_BIRTH\"])\n",
    "    train[\"begin_month\"] = np.abs(train[\"begin_month\"]).astype(int)\n",
    "    test[\"begin_month\"] = np.abs(test[\"begin_month\"]).astype(int)\n",
    "\n",
    "    # income_total\n",
    "    train = category_income(train)\n",
    "    test = category_income(test)\n",
    "\n",
    "    # DAYS_BIRTH\n",
    "    train[\"DAYS_BIRTH_month\"] = np.floor(train[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_month\"] = train[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    train[\"DAYS_BIRTH_week\"] = np.floor(train[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_BIRTH_week\"] = train[\"DAYS_BIRTH_week\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_month\"] = np.floor(test[\"DAYS_BIRTH\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_month\"] = test[\"DAYS_BIRTH_month\"].astype(int)\n",
    "    test[\"DAYS_BIRTH_week\"] = np.floor(test[\"DAYS_BIRTH\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_BIRTH\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_BIRTH_week\"] = test[\"DAYS_BIRTH_week\"].astype(int)\n",
    "\n",
    "    # Age\n",
    "    train[\"Age\"] = np.abs(train[\"DAYS_BIRTH\"]) // 360\n",
    "    test[\"Age\"] = np.abs(test[\"DAYS_BIRTH\"]) // 360\n",
    "\n",
    "    # DAYS_EMPLOYED\n",
    "    train[\"DAYS_EMPLOYED_month\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_month\"] = train[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    train[\"DAYS_EMPLOYED_week\"] = np.floor(train[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"DAYS_EMPLOYED_week\"] = train[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_month\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_month\"] = test[\"DAYS_EMPLOYED_month\"].astype(int)\n",
    "    test[\"DAYS_EMPLOYED_week\"] = np.floor(test[\"DAYS_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"DAYS_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"DAYS_EMPLOYED_week\"] = test[\"DAYS_EMPLOYED_week\"].astype(int)\n",
    "\n",
    "    # EMPLOYED\n",
    "    train[\"EMPLOYED\"] = train[\"DAYS_EMPLOYED\"] / 360\n",
    "    test[\"EMPLOYED\"] = test[\"DAYS_EMPLOYED\"] / 360\n",
    "\n",
    "    # before_EMPLOYED\n",
    "    train[\"before_EMPLOYED\"] = train[\"DAYS_BIRTH\"] - train[\"DAYS_EMPLOYED\"]\n",
    "    train[\"before_EMPLOYED_month\"] = np.floor(train[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    train[\"before_EMPLOYED_month\"] = train[\"before_EMPLOYED_month\"].astype(int)\n",
    "    train[\"before_EMPLOYED_week\"] = np.floor(train[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(train[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    train[\"before_EMPLOYED_week\"] = train[\"before_EMPLOYED_week\"].astype(int)\n",
    "    test[\"before_EMPLOYED\"] = test[\"DAYS_BIRTH\"] - test[\"DAYS_EMPLOYED\"]\n",
    "    test[\"before_EMPLOYED_month\"] = np.floor(test[\"before_EMPLOYED\"] / 30) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 30) / 12).astype(int) * 12\n",
    "    )\n",
    "    test[\"before_EMPLOYED_month\"] = test[\"before_EMPLOYED_month\"].astype(int)\n",
    "    test[\"before_EMPLOYED_week\"] = np.floor(test[\"before_EMPLOYED\"] / 7) - (\n",
    "        (np.floor(test[\"before_EMPLOYED\"] / 7) / 4).astype(int) * 4\n",
    "    )\n",
    "    test[\"before_EMPLOYED_week\"] = test[\"before_EMPLOYED_week\"].astype(int)\n",
    "\n",
    "    # gender_car_reality\n",
    "    train[\"user_code\"] = (\n",
    "        train[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + train[\"reality\"].astype(str)\n",
    "    )\n",
    "    test[\"user_code\"] = (\n",
    "        test[\"gender\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"car\"].astype(str)\n",
    "        + \"_\"\n",
    "        + test[\"reality\"].astype(str)\n",
    "    )\n",
    "\n",
    "    del_cols = [\n",
    "        \"gender\",\n",
    "        \"car\",\n",
    "        \"reality\",\n",
    "        \"email\",\n",
    "        \"child_num\",\n",
    "        \"DAYS_BIRTH\",\n",
    "        \"DAYS_EMPLOYED\",\n",
    "    ]\n",
    "    train.drop(train.loc[train[\"family_size\"] > 7, \"family_size\"].index, inplace=True)\n",
    "    train.drop(del_cols, axis=1, inplace=True)\n",
    "    test.drop(del_cols, axis=1, inplace=True)\n",
    "\n",
    "    cat_cols = [\n",
    "        \"income_type\",\n",
    "        \"edu_type\",\n",
    "        \"family_type\",\n",
    "        \"house_type\",\n",
    "        \"occyp_type\",\n",
    "        \"user_code\",\n",
    "    ]\n",
    "\n",
    "    for col in cat_cols:\n",
    "        label_encoder = LabelEncoder()\n",
    "        label_encoder = label_encoder.fit(train[col])\n",
    "        train[col] = label_encoder.transform(train[col])\n",
    "        test[col] = label_encoder.transform(test[col])\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:01.949594Z",
     "iopub.status.busy": "2021-05-24T03:18:01.949262Z",
     "iopub.status.idle": "2021-05-24T03:18:01.977430Z",
     "shell.execute_reply": "2021-05-24T03:18:01.976562Z",
     "shell.execute_reply.started": "2021-05-24T03:18:01.949559Z"
    }
   },
   "outputs": [],
   "source": [
    "# Catboost\n",
    "def stratified_kfold_cat(\n",
    "    params: Dict[str, Union[int, float, str, List[str]]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    cat_oof = np.zeros((X.shape[0], 3))\n",
    "    cat_preds = np.zeros((X_test.shape[0], 3))\n",
    "    cat_cols = [c for c in X.columns if X[c].dtypes == \"int64\"]\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        train_data = Pool(data=X_train, label=y_train, cat_features=cat_cols)\n",
    "        valid_data = Pool(data=X_valid, label=y_valid, cat_features=cat_cols)\n",
    "\n",
    "        model = CatBoostClassifier(**params)\n",
    "\n",
    "        model.fit(\n",
    "            train_data,\n",
    "            eval_set=valid_data,\n",
    "            early_stopping_rounds=100,\n",
    "            use_best_model=True,\n",
    "            verbose=100,\n",
    "        )\n",
    "\n",
    "        cat_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        cat_preds += model.predict_proba(X_test) / n_fold\n",
    "\n",
    "    log_score = log_loss(y, cat_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\\n\")\n",
    "    return cat_oof, cat_preds\n",
    "\n",
    "\n",
    "# Light GBM\n",
    "def stratified_kfold_lgbm(\n",
    "    params: Dict[str, Union[int, float, str]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    lgb_oof = np.zeros((X.shape[0], 3))\n",
    "    lgb_preds = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        pre_model = LGBMClassifier(**params)\n",
    "\n",
    "        pre_model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100,\n",
    "        )\n",
    "        params2 = params.copy()\n",
    "        params2[\"learning_rate\"] = params[\"learning_rate\"] * 0.1\n",
    "\n",
    "        model = LGBMClassifier(**params2)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100,\n",
    "            init_model=pre_model,\n",
    "        )\n",
    "        lgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        lgb_preds += model.predict_proba(X_test) / n_fold\n",
    "\n",
    "    log_score = log_loss(y, lgb_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
    "\n",
    "    return lgb_oof, lgb_preds\n",
    "\n",
    "\n",
    "# XGB\n",
    "def stratified_kfold_xgb(\n",
    "    params: Dict[str, Union[int, float, str]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    xgb_oof = np.zeros((X.shape[0], 3))\n",
    "    xgb_preds = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "            eval_set=[(X_train, y_train), (X_valid, y_valid)],\n",
    "            early_stopping_rounds=100,\n",
    "            verbose=100,\n",
    "        )\n",
    "\n",
    "        xgb_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        xgb_preds += model.predict_proba(X_test) / n_fold\n",
    "\n",
    "    log_score = log_loss(y, xgb_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
    "\n",
    "    return xgb_oof, xgb_preds\n",
    "\n",
    "\n",
    "# Random Foreset\n",
    "def stratified_kfold_rf(\n",
    "    params: Dict[str, Union[int, float, str, bool]],\n",
    "    n_fold: int,\n",
    "    X: pd.DataFrame,\n",
    "    y: pd.DataFrame,\n",
    "    X_test: pd.DataFrame,\n",
    ") -> Tuple[np.ndarray, np.ndarray]:\n",
    "\n",
    "    folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "    splits = folds.split(X, y)\n",
    "    rf_oof = np.zeros((X.shape[0], 3))\n",
    "    rf_preds = np.zeros((X_test.shape[0], 3))\n",
    "\n",
    "    for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "        print(f\"============ Fold {fold} ============\\n\")\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        model = RandomForestClassifier(**params)\n",
    "        model.fit(\n",
    "            X_train,\n",
    "            y_train,\n",
    "        )\n",
    "\n",
    "        rf_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "        rf_preds += model.predict_proba(X_test) / n_fold\n",
    "        print(f\"Log Loss Score: {log_loss(y_valid, rf_oof[valid_idx]):.5f}\")\n",
    "\n",
    "    log_score = log_loss(y, rf_oof)\n",
    "    print(f\"Log Loss Score: {log_score:.5f}\")\n",
    "\n",
    "    return rf_oof, rf_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cat Train\n",
    "+ catboost 같은 경우 categorical한 feature를 고정시켜주는게 핵심이라고 생각하여 하이퍼파라미터튜닝에 신경을 씀\n",
    "+ 하이퍼파라미터튜닝은 optuna 라이브러리로 했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:01.979135Z",
     "iopub.status.busy": "2021-05-24T03:18:01.978709Z",
     "iopub.status.idle": "2021-05-24T03:18:02.424836Z",
     "shell.execute_reply": "2021-05-24T03:18:02.424030Z",
     "shell.execute_reply.started": "2021-05-24T03:18:01.979068Z"
    }
   },
   "outputs": [],
   "source": [
    "train_cat, test_cat = cat_load_dataset()\n",
    "X = train_cat.drop(\"credit\", axis=1)\n",
    "y = train_cat[\"credit\"]\n",
    "\n",
    "X_test = test_cat.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T03:18:02.426466Z",
     "iopub.status.busy": "2021-05-24T03:18:02.426103Z",
     "iopub.status.idle": "2021-05-24T04:38:25.616125Z",
     "shell.execute_reply": "2021-05-24T04:38:25.615056Z",
     "shell.execute_reply.started": "2021-05-24T03:18:02.426429Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Fold 0 ============\n",
      "\n",
      "0:\tlearn: 1.0832640\ttest: 1.0829576\tbest: 1.0829576 (0)\ttotal: 301ms\tremaining: 50m 14s\n",
      "100:\tlearn: 0.7323907\ttest: 0.6929501\tbest: 0.6929501 (100)\ttotal: 37.1s\tremaining: 1h 37s\n",
      "200:\tlearn: 0.7058060\ttest: 0.6735975\tbest: 0.6735975 (200)\ttotal: 1m 27s\tremaining: 1h 10m 51s\n",
      "300:\tlearn: 0.6885144\ttest: 0.6706689\tbest: 0.6706689 (300)\ttotal: 2m 22s\tremaining: 1h 16m 18s\n",
      "400:\tlearn: 0.6701975\ttest: 0.6682080\tbest: 0.6681988 (388)\ttotal: 3m 16s\tremaining: 1h 18m 16s\n",
      "500:\tlearn: 0.6494751\ttest: 0.6664689\tbest: 0.6662605 (486)\ttotal: 4m 11s\tremaining: 1h 19m 23s\n",
      "600:\tlearn: 0.6290599\ttest: 0.6657848\tbest: 0.6657848 (600)\ttotal: 5m 6s\tremaining: 1h 19m 56s\n",
      "700:\tlearn: 0.6080674\ttest: 0.6648291\tbest: 0.6648201 (698)\ttotal: 6m 2s\tremaining: 1h 20m 7s\n",
      "800:\tlearn: 0.5895904\ttest: 0.6646412\tbest: 0.6643652 (761)\ttotal: 6m 58s\tremaining: 1h 20m\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6643651535\n",
      "bestIteration = 761\n",
      "\n",
      "Shrink model to first 762 iterations.\n",
      "============ Fold 1 ============\n",
      "\n",
      "0:\tlearn: 1.0831624\ttest: 1.0831592\tbest: 1.0831592 (0)\ttotal: 245ms\tremaining: 40m 51s\n",
      "100:\tlearn: 0.7348941\ttest: 0.6976274\tbest: 0.6976274 (100)\ttotal: 38.9s\tremaining: 1h 3m 36s\n",
      "200:\tlearn: 0.7091219\ttest: 0.6726237\tbest: 0.6726237 (200)\ttotal: 1m 29s\tremaining: 1h 12m 45s\n",
      "300:\tlearn: 0.6922515\ttest: 0.6687022\tbest: 0.6687022 (300)\ttotal: 2m 23s\tremaining: 1h 16m 51s\n",
      "400:\tlearn: 0.6724169\ttest: 0.6655868\tbest: 0.6655868 (400)\ttotal: 3m 19s\tremaining: 1h 19m 27s\n",
      "500:\tlearn: 0.6510442\ttest: 0.6630204\tbest: 0.6630204 (500)\ttotal: 4m 15s\tremaining: 1h 20m 48s\n",
      "600:\tlearn: 0.6316988\ttest: 0.6614526\tbest: 0.6614526 (600)\ttotal: 5m 11s\tremaining: 1h 21m 7s\n",
      "700:\tlearn: 0.6129803\ttest: 0.6607424\tbest: 0.6606660 (687)\ttotal: 6m 6s\tremaining: 1h 21m 6s\n",
      "800:\tlearn: 0.5930475\ttest: 0.6602049\tbest: 0.6601429 (793)\ttotal: 7m 3s\tremaining: 1h 21m\n",
      "900:\tlearn: 0.5758447\ttest: 0.6597748\tbest: 0.6596636 (885)\ttotal: 7m 59s\tremaining: 1h 20m 44s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6596635705\n",
      "bestIteration = 885\n",
      "\n",
      "Shrink model to first 886 iterations.\n",
      "============ Fold 2 ============\n",
      "\n",
      "0:\tlearn: 1.0831179\ttest: 1.0832346\tbest: 1.0832346 (0)\ttotal: 252ms\tremaining: 42m 1s\n",
      "100:\tlearn: 0.7276329\ttest: 0.6945366\tbest: 0.6945366 (100)\ttotal: 40.6s\tremaining: 1h 6m 18s\n",
      "200:\tlearn: 0.7020572\ttest: 0.6763344\tbest: 0.6763344 (200)\ttotal: 1m 35s\tremaining: 1h 17m 11s\n",
      "300:\tlearn: 0.6849021\ttest: 0.6734732\tbest: 0.6734732 (300)\ttotal: 2m 27s\tremaining: 1h 19m 24s\n",
      "400:\tlearn: 0.6678492\ttest: 0.6713501\tbest: 0.6713501 (400)\ttotal: 3m 21s\tremaining: 1h 20m 34s\n",
      "500:\tlearn: 0.6489240\ttest: 0.6697702\tbest: 0.6697702 (500)\ttotal: 4m 18s\tremaining: 1h 21m 32s\n",
      "600:\tlearn: 0.6286190\ttest: 0.6682808\tbest: 0.6682423 (598)\ttotal: 5m 13s\tremaining: 1h 21m 46s\n",
      "700:\tlearn: 0.6096438\ttest: 0.6678665\tbest: 0.6678645 (698)\ttotal: 6m 9s\tremaining: 1h 21m 46s\n",
      "800:\tlearn: 0.5927210\ttest: 0.6675491\tbest: 0.6673454 (766)\ttotal: 7m 5s\tremaining: 1h 21m 23s\n",
      "900:\tlearn: 0.5757733\ttest: 0.6672852\tbest: 0.6670491 (844)\ttotal: 8m\tremaining: 1h 20m 52s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6670490964\n",
      "bestIteration = 844\n",
      "\n",
      "Shrink model to first 845 iterations.\n",
      "============ Fold 3 ============\n",
      "\n",
      "0:\tlearn: 1.0831514\ttest: 1.0830642\tbest: 1.0830642 (0)\ttotal: 249ms\tremaining: 41m 29s\n",
      "100:\tlearn: 0.7303778\ttest: 0.7092536\tbest: 0.7092536 (100)\ttotal: 38.9s\tremaining: 1h 3m 36s\n",
      "200:\tlearn: 0.7023113\ttest: 0.6888512\tbest: 0.6888512 (200)\ttotal: 1m 31s\tremaining: 1h 14m 10s\n",
      "300:\tlearn: 0.6853219\ttest: 0.6845243\tbest: 0.6845243 (300)\ttotal: 2m 25s\tremaining: 1h 18m 8s\n",
      "400:\tlearn: 0.6675973\ttest: 0.6826500\tbest: 0.6826052 (388)\ttotal: 3m 20s\tremaining: 1h 20m 1s\n",
      "500:\tlearn: 0.6481532\ttest: 0.6808902\tbest: 0.6807720 (498)\ttotal: 4m 15s\tremaining: 1h 20m 50s\n",
      "600:\tlearn: 0.6285784\ttest: 0.6800921\tbest: 0.6800914 (595)\ttotal: 5m 12s\tremaining: 1h 21m 20s\n",
      "700:\tlearn: 0.6083968\ttest: 0.6796023\tbest: 0.6794399 (663)\ttotal: 6m 7s\tremaining: 1h 21m 14s\n",
      "800:\tlearn: 0.5899674\ttest: 0.6789752\tbest: 0.6788943 (792)\ttotal: 7m 2s\tremaining: 1h 20m 57s\n",
      "900:\tlearn: 0.5721327\ttest: 0.6783820\tbest: 0.6783820 (900)\ttotal: 7m 59s\tremaining: 1h 20m 38s\n",
      "1000:\tlearn: 0.5544633\ttest: 0.6780686\tbest: 0.6779982 (942)\ttotal: 8m 53s\tremaining: 1h 19m 59s\n",
      "1100:\tlearn: 0.5367893\ttest: 0.6780652\tbest: 0.6776773 (1037)\ttotal: 9m 49s\tremaining: 1h 19m 28s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6776773442\n",
      "bestIteration = 1037\n",
      "\n",
      "Shrink model to first 1038 iterations.\n",
      "============ Fold 4 ============\n",
      "\n",
      "0:\tlearn: 1.0831773\ttest: 1.0829793\tbest: 1.0829793 (0)\ttotal: 243ms\tremaining: 40m 32s\n",
      "100:\tlearn: 0.7273797\ttest: 0.6978671\tbest: 0.6978671 (100)\ttotal: 42.9s\tremaining: 1h 10m 1s\n",
      "200:\tlearn: 0.7017828\ttest: 0.6804129\tbest: 0.6804129 (200)\ttotal: 1m 35s\tremaining: 1h 17m 49s\n",
      "300:\tlearn: 0.6860989\ttest: 0.6769156\tbest: 0.6768962 (299)\ttotal: 2m 29s\tremaining: 1h 20m 5s\n",
      "400:\tlearn: 0.6681431\ttest: 0.6751760\tbest: 0.6751278 (397)\ttotal: 3m 24s\tremaining: 1h 21m 32s\n",
      "500:\tlearn: 0.6485897\ttest: 0.6737932\tbest: 0.6737814 (499)\ttotal: 4m 19s\tremaining: 1h 22m\n",
      "600:\tlearn: 0.6285877\ttest: 0.6730592\tbest: 0.6728910 (598)\ttotal: 5m 15s\tremaining: 1h 22m 18s\n",
      "700:\tlearn: 0.6097368\ttest: 0.6721781\tbest: 0.6721426 (699)\ttotal: 6m 10s\tremaining: 1h 21m 53s\n",
      "800:\tlearn: 0.5926144\ttest: 0.6721474\tbest: 0.6719096 (776)\ttotal: 7m 6s\tremaining: 1h 21m 34s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6719096303\n",
      "bestIteration = 776\n",
      "\n",
      "Shrink model to first 777 iterations.\n",
      "============ Fold 5 ============\n",
      "\n",
      "0:\tlearn: 1.0832186\ttest: 1.0833499\tbest: 1.0833499 (0)\ttotal: 243ms\tremaining: 40m 30s\n",
      "100:\tlearn: 0.7249238\ttest: 0.7056807\tbest: 0.7056807 (100)\ttotal: 39.7s\tremaining: 1h 4m 51s\n",
      "200:\tlearn: 0.6995494\ttest: 0.6906425\tbest: 0.6906425 (200)\ttotal: 1m 31s\tremaining: 1h 14m 12s\n",
      "300:\tlearn: 0.6821473\ttest: 0.6874748\tbest: 0.6874748 (300)\ttotal: 2m 24s\tremaining: 1h 17m 44s\n",
      "400:\tlearn: 0.6642446\ttest: 0.6860589\tbest: 0.6860589 (400)\ttotal: 3m 18s\tremaining: 1h 19m 19s\n",
      "500:\tlearn: 0.6449000\ttest: 0.6855384\tbest: 0.6855013 (499)\ttotal: 4m 14s\tremaining: 1h 20m 16s\n",
      "600:\tlearn: 0.6241052\ttest: 0.6848923\tbest: 0.6848844 (589)\ttotal: 5m 9s\tremaining: 1h 20m 40s\n",
      "700:\tlearn: 0.6055032\ttest: 0.6847242\tbest: 0.6844358 (666)\ttotal: 6m 4s\tremaining: 1h 20m 37s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6844358101\n",
      "bestIteration = 666\n",
      "\n",
      "Shrink model to first 667 iterations.\n",
      "============ Fold 6 ============\n",
      "\n",
      "0:\tlearn: 1.0831277\ttest: 1.0830831\tbest: 1.0830831 (0)\ttotal: 249ms\tremaining: 41m 28s\n",
      "100:\tlearn: 0.7299032\ttest: 0.7012839\tbest: 0.7012839 (100)\ttotal: 41.7s\tremaining: 1h 8m 3s\n",
      "200:\tlearn: 0.7043960\ttest: 0.6834182\tbest: 0.6833724 (188)\ttotal: 1m 34s\tremaining: 1h 16m 59s\n",
      "300:\tlearn: 0.6855325\ttest: 0.6793321\tbest: 0.6793321 (300)\ttotal: 2m 27s\tremaining: 1h 19m 28s\n",
      "400:\tlearn: 0.6663620\ttest: 0.6766051\tbest: 0.6766051 (400)\ttotal: 3m 22s\tremaining: 1h 20m 55s\n",
      "500:\tlearn: 0.6459442\ttest: 0.6751095\tbest: 0.6750316 (451)\ttotal: 4m 17s\tremaining: 1h 21m 29s\n",
      "600:\tlearn: 0.6271073\ttest: 0.6742075\tbest: 0.6741912 (599)\ttotal: 5m 13s\tremaining: 1h 21m 41s\n",
      "700:\tlearn: 0.6080896\ttest: 0.6741347\tbest: 0.6738342 (628)\ttotal: 6m 9s\tremaining: 1h 21m 41s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6738341764\n",
      "bestIteration = 628\n",
      "\n",
      "Shrink model to first 629 iterations.\n",
      "============ Fold 7 ============\n",
      "\n",
      "0:\tlearn: 1.0832149\ttest: 1.0831962\tbest: 1.0831962 (0)\ttotal: 241ms\tremaining: 40m 8s\n",
      "100:\tlearn: 0.7276994\ttest: 0.7009006\tbest: 0.7009006 (100)\ttotal: 40.2s\tremaining: 1h 5m 35s\n",
      "200:\tlearn: 0.7030936\ttest: 0.6848146\tbest: 0.6847949 (198)\ttotal: 1m 33s\tremaining: 1h 16m\n",
      "300:\tlearn: 0.6856515\ttest: 0.6815006\tbest: 0.6815006 (300)\ttotal: 2m 27s\tremaining: 1h 19m 5s\n",
      "400:\tlearn: 0.6681527\ttest: 0.6796503\tbest: 0.6795603 (384)\ttotal: 3m 22s\tremaining: 1h 20m 38s\n",
      "500:\tlearn: 0.6503799\ttest: 0.6787021\tbest: 0.6785460 (493)\ttotal: 4m 17s\tremaining: 1h 21m 19s\n",
      "600:\tlearn: 0.6319088\ttest: 0.6780125\tbest: 0.6779053 (597)\ttotal: 5m 12s\tremaining: 1h 21m 34s\n",
      "700:\tlearn: 0.6122103\ttest: 0.6774097\tbest: 0.6773104 (698)\ttotal: 6m 8s\tremaining: 1h 21m 28s\n",
      "800:\tlearn: 0.5939343\ttest: 0.6764882\tbest: 0.6764710 (799)\ttotal: 7m 5s\tremaining: 1h 21m 23s\n",
      "900:\tlearn: 0.5762685\ttest: 0.6764979\tbest: 0.6762442 (823)\ttotal: 8m 1s\tremaining: 1h 21m 3s\n",
      "1000:\tlearn: 0.5586911\ttest: 0.6759459\tbest: 0.6759226 (999)\ttotal: 8m 57s\tremaining: 1h 20m 29s\n",
      "1100:\tlearn: 0.5415433\ttest: 0.6763338\tbest: 0.6757223 (1032)\ttotal: 9m 53s\tremaining: 1h 19m 53s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6757223098\n",
      "bestIteration = 1032\n",
      "\n",
      "Shrink model to first 1033 iterations.\n",
      "============ Fold 8 ============\n",
      "\n",
      "0:\tlearn: 1.0831574\ttest: 1.0832749\tbest: 1.0832749 (0)\ttotal: 244ms\tremaining: 40m 44s\n",
      "100:\tlearn: 0.7315047\ttest: 0.7031190\tbest: 0.7031190 (100)\ttotal: 36s\tremaining: 58m 48s\n",
      "200:\tlearn: 0.7056228\ttest: 0.6837846\tbest: 0.6837846 (200)\ttotal: 1m 27s\tremaining: 1h 11m 24s\n",
      "300:\tlearn: 0.6886137\ttest: 0.6786870\tbest: 0.6786859 (299)\ttotal: 2m 21s\tremaining: 1h 15m 51s\n",
      "400:\tlearn: 0.6690231\ttest: 0.6763612\tbest: 0.6762533 (394)\ttotal: 3m 15s\tremaining: 1h 18m 8s\n",
      "500:\tlearn: 0.6483094\ttest: 0.6741831\tbest: 0.6741430 (497)\ttotal: 4m 10s\tremaining: 1h 19m 9s\n",
      "600:\tlearn: 0.6292264\ttest: 0.6731320\tbest: 0.6731320 (600)\ttotal: 5m 6s\tremaining: 1h 19m 51s\n",
      "700:\tlearn: 0.6111467\ttest: 0.6726734\tbest: 0.6726604 (662)\ttotal: 6m 1s\tremaining: 1h 19m 56s\n",
      "800:\tlearn: 0.5925022\ttest: 0.6727895\tbest: 0.6724140 (727)\ttotal: 6m 55s\tremaining: 1h 19m 34s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.672413971\n",
      "bestIteration = 727\n",
      "\n",
      "Shrink model to first 728 iterations.\n",
      "============ Fold 9 ============\n",
      "\n",
      "0:\tlearn: 1.0832009\ttest: 1.0827439\tbest: 1.0827439 (0)\ttotal: 241ms\tremaining: 40m 13s\n",
      "100:\tlearn: 0.7324903\ttest: 0.7048082\tbest: 0.7048082 (100)\ttotal: 35.7s\tremaining: 58m 16s\n",
      "200:\tlearn: 0.7065402\ttest: 0.6859564\tbest: 0.6859419 (198)\ttotal: 1m 24s\tremaining: 1h 8m 35s\n",
      "300:\tlearn: 0.6876098\ttest: 0.6816683\tbest: 0.6816683 (300)\ttotal: 2m 18s\tremaining: 1h 14m 13s\n",
      "400:\tlearn: 0.6702645\ttest: 0.6800155\tbest: 0.6800155 (400)\ttotal: 3m 13s\tremaining: 1h 17m 18s\n",
      "500:\tlearn: 0.6496183\ttest: 0.6780545\tbest: 0.6779703 (494)\ttotal: 4m 7s\tremaining: 1h 18m 21s\n",
      "600:\tlearn: 0.6291321\ttest: 0.6770233\tbest: 0.6769609 (596)\ttotal: 5m 3s\tremaining: 1h 19m 4s\n",
      "700:\tlearn: 0.6095260\ttest: 0.6765122\tbest: 0.6765086 (674)\ttotal: 5m 58s\tremaining: 1h 19m 18s\n",
      "800:\tlearn: 0.5920527\ttest: 0.6764972\tbest: 0.6763723 (712)\ttotal: 6m 53s\tremaining: 1h 19m 5s\n",
      "Stopped by overfitting detector  (100 iterations wait)\n",
      "\n",
      "bestTest = 0.6763722857\n",
      "bestIteration = 712\n",
      "\n",
      "Shrink model to first 713 iterations.\n",
      "Log Loss Score: 0.67234\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cat_params = {\n",
    "    \"learning_rate\": 0.026612467217016746,\n",
    "    \"l2_leaf_reg\": 0.3753065117824262,\n",
    "    \"max_depth\": 8,\n",
    "    \"bagging_temperature\": 1,\n",
    "    \"min_data_in_leaf\": 57,\n",
    "    \"max_bin\": 494,\n",
    "    \"random_state\": 42,\n",
    "    \"eval_metric\": \"MultiClass\",\n",
    "    \"loss_function\": \"MultiClass\",\n",
    "    \"od_type\": \"Iter\",\n",
    "    \"od_wait\": 500,\n",
    "    \"iterations\": 10000,\n",
    "    \"cat_features\": [\n",
    "        \"income_total\",\n",
    "        \"income_type\",\n",
    "        \"edu_type\",\n",
    "        \"family_type\",\n",
    "        \"house_type\",\n",
    "        \"FLAG_MOBIL\",\n",
    "        \"work_phone\",\n",
    "        \"phone\",\n",
    "        \"occyp_type\",\n",
    "        \"begin_month\",\n",
    "        \"DAYS_BIRTH_month\",\n",
    "        \"DAYS_BIRTH_week\",\n",
    "        \"Age\",\n",
    "        \"DAYS_EMPLOYED_month\",\n",
    "        \"DAYS_EMPLOYED_week\",\n",
    "        \"before_EMPLOYED\",\n",
    "        \"before_EMPLOYED_month\",\n",
    "        \"before_EMPLOYED_week\",\n",
    "        \"user_code\",\n",
    "    ],\n",
    "}\n",
    "\n",
    "cat_oof, cat_preds = stratified_kfold_cat(cat_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM Train\n",
    "+ 하이퍼파라미터 튜닝은 optuna를 사용했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:38:25.618271Z",
     "iopub.status.busy": "2021-05-24T04:38:25.617672Z",
     "iopub.status.idle": "2021-05-24T04:38:25.929996Z",
     "shell.execute_reply": "2021-05-24T04:38:25.929038Z",
     "shell.execute_reply.started": "2021-05-24T04:38:25.618225Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = load_dataset()\n",
    "X = train.drop(\"credit\", axis=1)\n",
    "y = train[\"credit\"]\n",
    "X_test = test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:38:25.932882Z",
     "iopub.status.busy": "2021-05-24T04:38:25.932517Z",
     "iopub.status.idle": "2021-05-24T04:41:27.262827Z",
     "shell.execute_reply": "2021-05-24T04:41:27.261743Z",
     "shell.execute_reply.started": "2021-05-24T04:38:25.932843Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Fold 0 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.558297\tvalid_1's multi_logloss: 0.710048\n",
      "[200]\ttraining's multi_logloss: 0.444307\tvalid_1's multi_logloss: 0.68103\n",
      "[300]\ttraining's multi_logloss: 0.370455\tvalid_1's multi_logloss: 0.675189\n",
      "[400]\ttraining's multi_logloss: 0.314399\tvalid_1's multi_logloss: 0.680516\n",
      "Early stopping, best iteration is:\n",
      "[311]\ttraining's multi_logloss: 0.364184\tvalid_1's multi_logloss: 0.674869\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[400]\ttraining's multi_logloss: 0.358725\tvalid_1's multi_logloss: 0.674764\n",
      "[500]\ttraining's multi_logloss: 0.352808\tvalid_1's multi_logloss: 0.67485\n",
      "Early stopping, best iteration is:\n",
      "[407]\ttraining's multi_logloss: 0.358272\tvalid_1's multi_logloss: 0.674724\n",
      "============ Fold 1 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.557051\tvalid_1's multi_logloss: 0.715119\n",
      "[200]\ttraining's multi_logloss: 0.442376\tvalid_1's multi_logloss: 0.688082\n",
      "[300]\ttraining's multi_logloss: 0.368424\tvalid_1's multi_logloss: 0.684491\n",
      "Early stopping, best iteration is:\n",
      "[286]\ttraining's multi_logloss: 0.37776\tvalid_1's multi_logloss: 0.683915\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.376758\tvalid_1's multi_logloss: 0.683944\n",
      "Early stopping, best iteration is:\n",
      "[293]\ttraining's multi_logloss: 0.37729\tvalid_1's multi_logloss: 0.683882\n",
      "============ Fold 2 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.559079\tvalid_1's multi_logloss: 0.709535\n",
      "[200]\ttraining's multi_logloss: 0.444743\tvalid_1's multi_logloss: 0.685369\n",
      "[300]\ttraining's multi_logloss: 0.370999\tvalid_1's multi_logloss: 0.68545\n",
      "Early stopping, best iteration is:\n",
      "[269]\ttraining's multi_logloss: 0.391609\tvalid_1's multi_logloss: 0.683632\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.389411\tvalid_1's multi_logloss: 0.683654\n",
      "[400]\ttraining's multi_logloss: 0.382514\tvalid_1's multi_logloss: 0.683673\n",
      "Early stopping, best iteration is:\n",
      "[368]\ttraining's multi_logloss: 0.384574\tvalid_1's multi_logloss: 0.68357\n",
      "============ Fold 3 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.55667\tvalid_1's multi_logloss: 0.72357\n",
      "[200]\ttraining's multi_logloss: 0.443354\tvalid_1's multi_logloss: 0.701939\n",
      "[300]\ttraining's multi_logloss: 0.369061\tvalid_1's multi_logloss: 0.701999\n",
      "Early stopping, best iteration is:\n",
      "[221]\ttraining's multi_logloss: 0.425229\tvalid_1's multi_logloss: 0.699529\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.418732\tvalid_1's multi_logloss: 0.699265\n",
      "[400]\ttraining's multi_logloss: 0.410996\tvalid_1's multi_logloss: 0.698926\n",
      "[500]\ttraining's multi_logloss: 0.403574\tvalid_1's multi_logloss: 0.69874\n",
      "[600]\ttraining's multi_logloss: 0.395812\tvalid_1's multi_logloss: 0.698748\n",
      "Early stopping, best iteration is:\n",
      "[508]\ttraining's multi_logloss: 0.403002\tvalid_1's multi_logloss: 0.698689\n",
      "============ Fold 4 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.559749\tvalid_1's multi_logloss: 0.709381\n",
      "[200]\ttraining's multi_logloss: 0.444327\tvalid_1's multi_logloss: 0.681738\n",
      "[300]\ttraining's multi_logloss: 0.369887\tvalid_1's multi_logloss: 0.677157\n",
      "Early stopping, best iteration is:\n",
      "[278]\ttraining's multi_logloss: 0.384456\tvalid_1's multi_logloss: 0.676522\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.382884\tvalid_1's multi_logloss: 0.67655\n",
      "Early stopping, best iteration is:\n",
      "[291]\ttraining's multi_logloss: 0.383463\tvalid_1's multi_logloss: 0.67649\n",
      "============ Fold 5 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.556635\tvalid_1's multi_logloss: 0.728675\n",
      "[200]\ttraining's multi_logloss: 0.440301\tvalid_1's multi_logloss: 0.705799\n",
      "[300]\ttraining's multi_logloss: 0.365431\tvalid_1's multi_logloss: 0.708614\n",
      "Early stopping, best iteration is:\n",
      "[249]\ttraining's multi_logloss: 0.399944\tvalid_1's multi_logloss: 0.703978\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.396066\tvalid_1's multi_logloss: 0.704059\n",
      "Early stopping, best iteration is:\n",
      "[256]\ttraining's multi_logloss: 0.399432\tvalid_1's multi_logloss: 0.703956\n",
      "============ Fold 6 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.558638\tvalid_1's multi_logloss: 0.718429\n",
      "[200]\ttraining's multi_logloss: 0.443756\tvalid_1's multi_logloss: 0.692409\n",
      "[300]\ttraining's multi_logloss: 0.370155\tvalid_1's multi_logloss: 0.687175\n",
      "Early stopping, best iteration is:\n",
      "[287]\ttraining's multi_logloss: 0.379327\tvalid_1's multi_logloss: 0.686592\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.37843\tvalid_1's multi_logloss: 0.686592\n",
      "[400]\ttraining's multi_logloss: 0.371816\tvalid_1's multi_logloss: 0.686755\n",
      "Early stopping, best iteration is:\n",
      "[320]\ttraining's multi_logloss: 0.37714\tvalid_1's multi_logloss: 0.686518\n",
      "============ Fold 7 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.558191\tvalid_1's multi_logloss: 0.716947\n",
      "[200]\ttraining's multi_logloss: 0.444287\tvalid_1's multi_logloss: 0.690699\n",
      "[300]\ttraining's multi_logloss: 0.368962\tvalid_1's multi_logloss: 0.689638\n",
      "Early stopping, best iteration is:\n",
      "[269]\ttraining's multi_logloss: 0.389597\tvalid_1's multi_logloss: 0.687518\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.387454\tvalid_1's multi_logloss: 0.687559\n",
      "Early stopping, best iteration is:\n",
      "[273]\ttraining's multi_logloss: 0.389322\tvalid_1's multi_logloss: 0.687514\n",
      "============ Fold 8 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.556743\tvalid_1's multi_logloss: 0.721669\n",
      "[200]\ttraining's multi_logloss: 0.441304\tvalid_1's multi_logloss: 0.700372\n",
      "[300]\ttraining's multi_logloss: 0.367715\tvalid_1's multi_logloss: 0.699101\n",
      "Early stopping, best iteration is:\n",
      "[266]\ttraining's multi_logloss: 0.390431\tvalid_1's multi_logloss: 0.697598\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.388073\tvalid_1's multi_logloss: 0.697634\n",
      "Early stopping, best iteration is:\n",
      "[267]\ttraining's multi_logloss: 0.390377\tvalid_1's multi_logloss: 0.697596\n",
      "============ Fold 9 ============\n",
      "\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[100]\ttraining's multi_logloss: 0.556094\tvalid_1's multi_logloss: 0.714247\n",
      "[200]\ttraining's multi_logloss: 0.442896\tvalid_1's multi_logloss: 0.692599\n",
      "[300]\ttraining's multi_logloss: 0.368471\tvalid_1's multi_logloss: 0.691702\n",
      "Early stopping, best iteration is:\n",
      "[262]\ttraining's multi_logloss: 0.394171\tvalid_1's multi_logloss: 0.690039\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[300]\ttraining's multi_logloss: 0.391432\tvalid_1's multi_logloss: 0.690078\n",
      "Early stopping, best iteration is:\n",
      "[283]\ttraining's multi_logloss: 0.392614\tvalid_1's multi_logloss: 0.690004\n",
      "Log Loss Score: 0.68829\n"
     ]
    }
   ],
   "source": [
    "lgb_params = {\n",
    "    \"reg_alpha\": 5.998770177220496e-05,\n",
    "    \"reg_lambda\": 0.07127674208132959,\n",
    "    \"max_depth\": 18,\n",
    "    \"num_leaves\": 125,\n",
    "    \"colsample_bytree\": 0.4241631237880101,\n",
    "    \"subsample\": 0.8876057928391585,\n",
    "    \"subsample_freq\": 5,\n",
    "    \"min_child_samples\": 5,\n",
    "    \"max_bin\": 449,\n",
    "    \"random_state\": 42,\n",
    "    \"boosting_type\": \"gbdt\",\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"objective\": \"multiclass\",\n",
    "    \"metric\": \"multi_logloss\",\n",
    "}\n",
    "lgbm_oof, lgbm_preds = stratified_kfold_lgbm(lgb_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGB Train\n",
    "+ 하이퍼파라미터 튜닝을 optuna를 사용했음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:41:27.264935Z",
     "iopub.status.busy": "2021-05-24T04:41:27.264591Z",
     "iopub.status.idle": "2021-05-24T04:51:02.999834Z",
     "shell.execute_reply": "2021-05-24T04:51:02.999034Z",
     "shell.execute_reply.started": "2021-05-24T04:41:27.264897Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Fold 0 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08671\tvalidation_1-mlogloss:1.08830\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[100]\tvalidation_0-mlogloss:0.60860\tvalidation_1-mlogloss:0.73678\n",
      "[200]\tvalidation_0-mlogloss:0.49016\tvalidation_1-mlogloss:0.69367\n",
      "[300]\tvalidation_0-mlogloss:0.42604\tvalidation_1-mlogloss:0.68027\n",
      "[400]\tvalidation_0-mlogloss:0.38186\tvalidation_1-mlogloss:0.67709\n",
      "[497]\tvalidation_0-mlogloss:0.34856\tvalidation_1-mlogloss:0.67779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/xgboost/data.py:114: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  \"because it will generate extra copies and increase \" +\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Fold 1 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08610\tvalidation_1-mlogloss:1.08795\n",
      "[100]\tvalidation_0-mlogloss:0.61238\tvalidation_1-mlogloss:0.74311\n",
      "[200]\tvalidation_0-mlogloss:0.49265\tvalidation_1-mlogloss:0.70016\n",
      "[300]\tvalidation_0-mlogloss:0.42657\tvalidation_1-mlogloss:0.68767\n",
      "[400]\tvalidation_0-mlogloss:0.38175\tvalidation_1-mlogloss:0.68516\n",
      "[500]\tvalidation_0-mlogloss:0.34796\tvalidation_1-mlogloss:0.68706\n",
      "[518]\tvalidation_0-mlogloss:0.34252\tvalidation_1-mlogloss:0.68770\n",
      "============ Fold 2 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08659\tvalidation_1-mlogloss:1.08834\n",
      "[100]\tvalidation_0-mlogloss:0.61568\tvalidation_1-mlogloss:0.74170\n",
      "[200]\tvalidation_0-mlogloss:0.49399\tvalidation_1-mlogloss:0.69688\n",
      "[300]\tvalidation_0-mlogloss:0.42702\tvalidation_1-mlogloss:0.68586\n",
      "[400]\tvalidation_0-mlogloss:0.38326\tvalidation_1-mlogloss:0.68467\n",
      "[466]\tvalidation_0-mlogloss:0.35951\tvalidation_1-mlogloss:0.68507\n",
      "============ Fold 3 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08586\tvalidation_1-mlogloss:1.08814\n",
      "[100]\tvalidation_0-mlogloss:0.61298\tvalidation_1-mlogloss:0.74823\n",
      "[200]\tvalidation_0-mlogloss:0.49382\tvalidation_1-mlogloss:0.70872\n",
      "[300]\tvalidation_0-mlogloss:0.42853\tvalidation_1-mlogloss:0.70048\n",
      "[400]\tvalidation_0-mlogloss:0.38378\tvalidation_1-mlogloss:0.70023\n",
      "[456]\tvalidation_0-mlogloss:0.36341\tvalidation_1-mlogloss:0.70123\n",
      "============ Fold 4 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08610\tvalidation_1-mlogloss:1.08800\n",
      "[100]\tvalidation_0-mlogloss:0.61311\tvalidation_1-mlogloss:0.73966\n",
      "[200]\tvalidation_0-mlogloss:0.49023\tvalidation_1-mlogloss:0.69509\n",
      "[300]\tvalidation_0-mlogloss:0.42441\tvalidation_1-mlogloss:0.68346\n",
      "[400]\tvalidation_0-mlogloss:0.38015\tvalidation_1-mlogloss:0.68037\n",
      "[500]\tvalidation_0-mlogloss:0.34675\tvalidation_1-mlogloss:0.68140\n",
      "[514]\tvalidation_0-mlogloss:0.34254\tvalidation_1-mlogloss:0.68174\n",
      "============ Fold 5 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08681\tvalidation_1-mlogloss:1.08855\n",
      "[100]\tvalidation_0-mlogloss:0.61195\tvalidation_1-mlogloss:0.75309\n",
      "[200]\tvalidation_0-mlogloss:0.49143\tvalidation_1-mlogloss:0.71664\n",
      "[300]\tvalidation_0-mlogloss:0.42520\tvalidation_1-mlogloss:0.71000\n",
      "[400]\tvalidation_0-mlogloss:0.37887\tvalidation_1-mlogloss:0.71086\n",
      "[432]\tvalidation_0-mlogloss:0.36822\tvalidation_1-mlogloss:0.71160\n",
      "============ Fold 6 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08592\tvalidation_1-mlogloss:1.08840\n",
      "[100]\tvalidation_0-mlogloss:0.61311\tvalidation_1-mlogloss:0.74551\n",
      "[200]\tvalidation_0-mlogloss:0.49330\tvalidation_1-mlogloss:0.70115\n",
      "[300]\tvalidation_0-mlogloss:0.42652\tvalidation_1-mlogloss:0.68875\n",
      "[400]\tvalidation_0-mlogloss:0.38145\tvalidation_1-mlogloss:0.68545\n",
      "[500]\tvalidation_0-mlogloss:0.34812\tvalidation_1-mlogloss:0.68636\n",
      "[511]\tvalidation_0-mlogloss:0.34473\tvalidation_1-mlogloss:0.68679\n",
      "============ Fold 7 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08657\tvalidation_1-mlogloss:1.08863\n",
      "[100]\tvalidation_0-mlogloss:0.61477\tvalidation_1-mlogloss:0.74560\n",
      "[200]\tvalidation_0-mlogloss:0.49598\tvalidation_1-mlogloss:0.70359\n",
      "[300]\tvalidation_0-mlogloss:0.42721\tvalidation_1-mlogloss:0.69131\n",
      "[400]\tvalidation_0-mlogloss:0.38060\tvalidation_1-mlogloss:0.68940\n",
      "[500]\tvalidation_0-mlogloss:0.34677\tvalidation_1-mlogloss:0.69192\n",
      "============ Fold 8 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08615\tvalidation_1-mlogloss:1.08787\n",
      "[100]\tvalidation_0-mlogloss:0.61230\tvalidation_1-mlogloss:0.74758\n",
      "[200]\tvalidation_0-mlogloss:0.49440\tvalidation_1-mlogloss:0.70954\n",
      "[300]\tvalidation_0-mlogloss:0.42756\tvalidation_1-mlogloss:0.70042\n",
      "[400]\tvalidation_0-mlogloss:0.38250\tvalidation_1-mlogloss:0.69977\n",
      "[489]\tvalidation_0-mlogloss:0.35041\tvalidation_1-mlogloss:0.70161\n",
      "============ Fold 9 ============\n",
      "\n",
      "[0]\tvalidation_0-mlogloss:1.08661\tvalidation_1-mlogloss:1.08835\n",
      "[100]\tvalidation_0-mlogloss:0.61319\tvalidation_1-mlogloss:0.74565\n",
      "[200]\tvalidation_0-mlogloss:0.49023\tvalidation_1-mlogloss:0.70382\n",
      "[300]\tvalidation_0-mlogloss:0.42508\tvalidation_1-mlogloss:0.69486\n",
      "[400]\tvalidation_0-mlogloss:0.38028\tvalidation_1-mlogloss:0.69438\n",
      "[442]\tvalidation_0-mlogloss:0.36562\tvalidation_1-mlogloss:0.69537\n",
      "Log Loss Score: 0.69034\n"
     ]
    }
   ],
   "source": [
    "xgb_params = {\n",
    "    \"eta\": 0.023839252347297356,\n",
    "    \"reg_alpha\": 6.99554614267605e-06,\n",
    "    \"reg_lambda\": 0.010419988953061583,\n",
    "    \"max_depth\": 15,\n",
    "    \"max_leaves\": 159,\n",
    "    \"colsample_bytree\": 0.4515469593932409,\n",
    "    \"subsample\": 0.7732694309118915,\n",
    "    \"min_child_weight\": 5,\n",
    "    \"gamma\": 0.6847131315687576,\n",
    "    \"random_state\": 42,\n",
    "    \"n_estimators\": 10000,\n",
    "    \"objective\": \"multi:softmax\",\n",
    "    \"eval_metric\": \"mlogloss\",\n",
    "}\n",
    "xgb_oof, xgb_preds = stratified_kfold_xgb(xgb_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForest Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:51:03.001643Z",
     "iopub.status.busy": "2021-05-24T04:51:03.001315Z",
     "iopub.status.idle": "2021-05-24T04:52:28.223881Z",
     "shell.execute_reply": "2021-05-24T04:52:28.223049Z",
     "shell.execute_reply.started": "2021-05-24T04:51:03.001606Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Fold 0 ============\n",
      "\n",
      "Log Loss Score: 0.68083\n",
      "============ Fold 1 ============\n",
      "\n",
      "Log Loss Score: 0.68767\n",
      "============ Fold 2 ============\n",
      "\n",
      "Log Loss Score: 0.68559\n",
      "============ Fold 3 ============\n",
      "\n",
      "Log Loss Score: 0.69447\n",
      "============ Fold 4 ============\n",
      "\n",
      "Log Loss Score: 0.68046\n",
      "============ Fold 5 ============\n",
      "\n",
      "Log Loss Score: 0.70892\n",
      "============ Fold 6 ============\n",
      "\n",
      "Log Loss Score: 0.69037\n",
      "============ Fold 7 ============\n",
      "\n",
      "Log Loss Score: 0.69148\n",
      "============ Fold 8 ============\n",
      "\n",
      "Log Loss Score: 0.69465\n",
      "============ Fold 9 ============\n",
      "\n",
      "Log Loss Score: 0.69926\n",
      "Log Loss Score: 0.69137\n"
     ]
    }
   ],
   "source": [
    "rf_params = {\n",
    "        \"criterion\": \"gini\",\n",
    "        \"n_estimators\": 300,\n",
    "        \"min_samples_split\": 10,\n",
    "        \"min_samples_leaf\": 2,\n",
    "        \"max_features\": \"auto\",\n",
    "        \"oob_score\": True,\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "rf_oof, rf_preds = stratified_kfold_rf(rf_params, 10, X, y, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.225718Z",
     "iopub.status.busy": "2021-05-24T04:52:28.225231Z",
     "iopub.status.idle": "2021-05-24T04:52:28.633099Z",
     "shell.execute_reply": "2021-05-24T04:52:28.632139Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.225678Z"
    }
   },
   "outputs": [],
   "source": [
    "train, test = load_dataset()\n",
    "train_x = train.drop(\"credit\", axis = 1)\n",
    "train_y = train['credit'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.634938Z",
     "iopub.status.busy": "2021-05-24T04:52:28.634352Z",
     "iopub.status.idle": "2021-05-24T04:52:28.642952Z",
     "shell.execute_reply": "2021-05-24T04:52:28.641787Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.634896Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26451, 12)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pred = np.concatenate([cat_oof, lgbm_oof, xgb_oof, rf_oof], axis=1)\n",
    "train_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.645045Z",
     "iopub.status.busy": "2021-05-24T04:52:28.644375Z",
     "iopub.status.idle": "2021-05-24T04:52:28.652160Z",
     "shell.execute_reply": "2021-05-24T04:52:28.651111Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.644976Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 12)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred = np.concatenate([cat_preds, lgbm_preds, xgb_preds, rf_preds], axis=1)\n",
    "test_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Tabular\n",
    "+ Stacking Ensemble을 사용하며 학습 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.654142Z",
     "iopub.status.busy": "2021-05-24T04:52:28.653618Z",
     "iopub.status.idle": "2021-05-24T04:52:28.659053Z",
     "shell.execute_reply": "2021-05-24T04:52:28.658271Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.654109Z"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:52:28.660957Z",
     "iopub.status.busy": "2021-05-24T04:52:28.660315Z",
     "iopub.status.idle": "2021-05-24T04:59:40.592964Z",
     "shell.execute_reply": "2021-05-24T04:59:40.591970Z",
     "shell.execute_reply.started": "2021-05-24T04:52:28.660920Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============ Fold 0 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.75703 | val_0_logloss: 0.79897 |  0:00:01s\n",
      "epoch 1  | loss: 0.68556 | val_0_logloss: 0.77978 |  0:00:02s\n",
      "epoch 2  | loss: 0.68048 | val_0_logloss: 0.75859 |  0:00:04s\n",
      "epoch 3  | loss: 0.67863 | val_0_logloss: 0.75964 |  0:00:05s\n",
      "epoch 4  | loss: 0.68124 | val_0_logloss: 0.75431 |  0:00:06s\n",
      "epoch 5  | loss: 0.67689 | val_0_logloss: 0.73965 |  0:00:07s\n",
      "epoch 6  | loss: 0.67523 | val_0_logloss: 0.7408  |  0:00:08s\n",
      "epoch 7  | loss: 0.67533 | val_0_logloss: 0.7257  |  0:00:10s\n",
      "epoch 8  | loss: 0.67628 | val_0_logloss: 0.7194  |  0:00:11s\n",
      "epoch 9  | loss: 0.67497 | val_0_logloss: 0.70544 |  0:00:12s\n",
      "epoch 10 | loss: 0.67502 | val_0_logloss: 0.71008 |  0:00:13s\n",
      "epoch 11 | loss: 0.67271 | val_0_logloss: 0.68738 |  0:00:14s\n",
      "epoch 12 | loss: 0.67292 | val_0_logloss: 0.68595 |  0:00:15s\n",
      "epoch 13 | loss: 0.67197 | val_0_logloss: 0.68977 |  0:00:17s\n",
      "epoch 14 | loss: 0.67286 | val_0_logloss: 0.66919 |  0:00:18s\n",
      "epoch 15 | loss: 0.67186 | val_0_logloss: 0.68656 |  0:00:19s\n",
      "epoch 16 | loss: 0.67059 | val_0_logloss: 0.66627 |  0:00:20s\n",
      "epoch 17 | loss: 0.67243 | val_0_logloss: 0.67682 |  0:00:21s\n",
      "epoch 18 | loss: 0.67134 | val_0_logloss: 0.67325 |  0:00:23s\n",
      "epoch 19 | loss: 0.67091 | val_0_logloss: 0.66412 |  0:00:24s\n",
      "epoch 20 | loss: 0.66908 | val_0_logloss: 0.66527 |  0:00:25s\n",
      "epoch 21 | loss: 0.66837 | val_0_logloss: 0.6693  |  0:00:26s\n",
      "epoch 22 | loss: 0.67108 | val_0_logloss: 0.66373 |  0:00:28s\n",
      "epoch 23 | loss: 0.67065 | val_0_logloss: 0.66239 |  0:00:29s\n",
      "epoch 24 | loss: 0.66828 | val_0_logloss: 0.66305 |  0:00:30s\n",
      "epoch 25 | loss: 0.66773 | val_0_logloss: 0.66454 |  0:00:31s\n",
      "epoch 26 | loss: 0.66873 | val_0_logloss: 0.66477 |  0:00:32s\n",
      "epoch 27 | loss: 0.67005 | val_0_logloss: 0.66604 |  0:00:33s\n",
      "epoch 28 | loss: 0.67029 | val_0_logloss: 0.66195 |  0:00:35s\n",
      "epoch 29 | loss: 0.66815 | val_0_logloss: 0.66411 |  0:00:36s\n",
      "epoch 30 | loss: 0.66782 | val_0_logloss: 0.66861 |  0:00:37s\n",
      "epoch 31 | loss: 0.6677  | val_0_logloss: 0.66263 |  0:00:38s\n",
      "epoch 32 | loss: 0.66615 | val_0_logloss: 0.66735 |  0:00:39s\n",
      "epoch 33 | loss: 0.66769 | val_0_logloss: 0.66755 |  0:00:41s\n",
      "epoch 34 | loss: 0.66673 | val_0_logloss: 0.66634 |  0:00:42s\n",
      "epoch 35 | loss: 0.66852 | val_0_logloss: 0.66559 |  0:00:43s\n",
      "epoch 36 | loss: 0.66838 | val_0_logloss: 0.66103 |  0:00:44s\n",
      "epoch 37 | loss: 0.66698 | val_0_logloss: 0.66733 |  0:00:45s\n",
      "epoch 38 | loss: 0.66684 | val_0_logloss: 0.67079 |  0:00:46s\n",
      "epoch 39 | loss: 0.66516 | val_0_logloss: 0.66535 |  0:00:48s\n",
      "epoch 40 | loss: 0.66591 | val_0_logloss: 0.66634 |  0:00:49s\n",
      "epoch 41 | loss: 0.66749 | val_0_logloss: 0.66472 |  0:00:50s\n",
      "epoch 42 | loss: 0.66719 | val_0_logloss: 0.6655  |  0:00:51s\n",
      "epoch 43 | loss: 0.66643 | val_0_logloss: 0.66642 |  0:00:52s\n",
      "epoch 44 | loss: 0.66607 | val_0_logloss: 0.66885 |  0:00:54s\n",
      "epoch 45 | loss: 0.66705 | val_0_logloss: 0.66945 |  0:00:55s\n",
      "epoch 46 | loss: 0.66741 | val_0_logloss: 0.66538 |  0:00:56s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_logloss = 0.66103\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 1 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.75784 | val_0_logloss: 0.79563 |  0:00:01s\n",
      "epoch 1  | loss: 0.68654 | val_0_logloss: 0.79613 |  0:00:02s\n",
      "epoch 2  | loss: 0.68146 | val_0_logloss: 0.79035 |  0:00:03s\n",
      "epoch 3  | loss: 0.6774  | val_0_logloss: 0.77993 |  0:00:05s\n",
      "epoch 4  | loss: 0.67687 | val_0_logloss: 0.78368 |  0:00:06s\n",
      "epoch 5  | loss: 0.67737 | val_0_logloss: 0.76388 |  0:00:07s\n",
      "epoch 6  | loss: 0.67758 | val_0_logloss: 0.74639 |  0:00:08s\n",
      "epoch 7  | loss: 0.67524 | val_0_logloss: 0.73145 |  0:00:09s\n",
      "epoch 8  | loss: 0.6746  | val_0_logloss: 0.72927 |  0:00:10s\n",
      "epoch 9  | loss: 0.67411 | val_0_logloss: 0.73216 |  0:00:12s\n",
      "epoch 10 | loss: 0.67581 | val_0_logloss: 0.71446 |  0:00:13s\n",
      "epoch 11 | loss: 0.67485 | val_0_logloss: 0.70395 |  0:00:14s\n",
      "epoch 12 | loss: 0.67382 | val_0_logloss: 0.69355 |  0:00:15s\n",
      "epoch 13 | loss: 0.67387 | val_0_logloss: 0.68349 |  0:00:16s\n",
      "epoch 14 | loss: 0.67322 | val_0_logloss: 0.67214 |  0:00:18s\n",
      "epoch 15 | loss: 0.67596 | val_0_logloss: 0.68645 |  0:00:19s\n",
      "epoch 16 | loss: 0.67294 | val_0_logloss: 0.6731  |  0:00:20s\n",
      "epoch 17 | loss: 0.6727  | val_0_logloss: 0.67529 |  0:00:21s\n",
      "epoch 18 | loss: 0.67198 | val_0_logloss: 0.66173 |  0:00:22s\n",
      "epoch 19 | loss: 0.67147 | val_0_logloss: 0.66399 |  0:00:23s\n",
      "epoch 20 | loss: 0.67127 | val_0_logloss: 0.67185 |  0:00:24s\n",
      "epoch 21 | loss: 0.67174 | val_0_logloss: 0.66423 |  0:00:26s\n",
      "epoch 22 | loss: 0.67167 | val_0_logloss: 0.66632 |  0:00:27s\n",
      "epoch 23 | loss: 0.6719  | val_0_logloss: 0.66194 |  0:00:28s\n",
      "epoch 24 | loss: 0.67189 | val_0_logloss: 0.67195 |  0:00:29s\n",
      "epoch 25 | loss: 0.67012 | val_0_logloss: 0.66625 |  0:00:30s\n",
      "epoch 26 | loss: 0.67111 | val_0_logloss: 0.66153 |  0:00:32s\n",
      "epoch 27 | loss: 0.67057 | val_0_logloss: 0.65844 |  0:00:33s\n",
      "epoch 28 | loss: 0.66896 | val_0_logloss: 0.66221 |  0:00:34s\n",
      "epoch 29 | loss: 0.66932 | val_0_logloss: 0.65973 |  0:00:35s\n",
      "epoch 30 | loss: 0.669   | val_0_logloss: 0.66793 |  0:00:37s\n",
      "epoch 31 | loss: 0.67002 | val_0_logloss: 0.66277 |  0:00:38s\n",
      "epoch 32 | loss: 0.66833 | val_0_logloss: 0.66818 |  0:00:39s\n",
      "epoch 33 | loss: 0.66887 | val_0_logloss: 0.65916 |  0:00:40s\n",
      "epoch 34 | loss: 0.66949 | val_0_logloss: 0.65934 |  0:00:41s\n",
      "epoch 35 | loss: 0.6688  | val_0_logloss: 0.66253 |  0:00:43s\n",
      "epoch 36 | loss: 0.67042 | val_0_logloss: 0.65917 |  0:00:44s\n",
      "epoch 37 | loss: 0.66809 | val_0_logloss: 0.66578 |  0:00:45s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_logloss = 0.65844\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 2 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.74781 | val_0_logloss: 0.78794 |  0:00:01s\n",
      "epoch 1  | loss: 0.68262 | val_0_logloss: 0.79278 |  0:00:02s\n",
      "epoch 2  | loss: 0.68062 | val_0_logloss: 0.79872 |  0:00:03s\n",
      "epoch 3  | loss: 0.67857 | val_0_logloss: 0.77633 |  0:00:04s\n",
      "epoch 4  | loss: 0.67567 | val_0_logloss: 0.75646 |  0:00:06s\n",
      "epoch 5  | loss: 0.67693 | val_0_logloss: 0.75099 |  0:00:07s\n",
      "epoch 6  | loss: 0.67673 | val_0_logloss: 0.73935 |  0:00:08s\n",
      "epoch 7  | loss: 0.67494 | val_0_logloss: 0.72426 |  0:00:09s\n",
      "epoch 8  | loss: 0.67348 | val_0_logloss: 0.70824 |  0:00:10s\n",
      "epoch 9  | loss: 0.67334 | val_0_logloss: 0.70252 |  0:00:11s\n",
      "epoch 10 | loss: 0.67364 | val_0_logloss: 0.69631 |  0:00:13s\n",
      "epoch 11 | loss: 0.67363 | val_0_logloss: 0.69507 |  0:00:14s\n",
      "epoch 12 | loss: 0.67425 | val_0_logloss: 0.6934  |  0:00:15s\n",
      "epoch 13 | loss: 0.67289 | val_0_logloss: 0.67599 |  0:00:16s\n",
      "epoch 14 | loss: 0.67264 | val_0_logloss: 0.67489 |  0:00:17s\n",
      "epoch 15 | loss: 0.67156 | val_0_logloss: 0.67268 |  0:00:19s\n",
      "epoch 16 | loss: 0.67047 | val_0_logloss: 0.67257 |  0:00:20s\n",
      "epoch 17 | loss: 0.67152 | val_0_logloss: 0.67261 |  0:00:21s\n",
      "epoch 18 | loss: 0.67153 | val_0_logloss: 0.66545 |  0:00:22s\n",
      "epoch 19 | loss: 0.67131 | val_0_logloss: 0.66737 |  0:00:24s\n",
      "epoch 20 | loss: 0.67075 | val_0_logloss: 0.66481 |  0:00:25s\n",
      "epoch 21 | loss: 0.6718  | val_0_logloss: 0.66641 |  0:00:26s\n",
      "epoch 22 | loss: 0.67053 | val_0_logloss: 0.66881 |  0:00:28s\n",
      "epoch 23 | loss: 0.66932 | val_0_logloss: 0.66834 |  0:00:29s\n",
      "epoch 24 | loss: 0.67098 | val_0_logloss: 0.66373 |  0:00:30s\n",
      "epoch 25 | loss: 0.67009 | val_0_logloss: 0.66402 |  0:00:31s\n",
      "epoch 26 | loss: 0.66896 | val_0_logloss: 0.67017 |  0:00:32s\n",
      "epoch 27 | loss: 0.6692  | val_0_logloss: 0.6662  |  0:00:33s\n",
      "epoch 28 | loss: 0.66822 | val_0_logloss: 0.66437 |  0:00:35s\n",
      "epoch 29 | loss: 0.66858 | val_0_logloss: 0.66511 |  0:00:36s\n",
      "epoch 30 | loss: 0.66701 | val_0_logloss: 0.6648  |  0:00:37s\n",
      "epoch 31 | loss: 0.667   | val_0_logloss: 0.6634  |  0:00:38s\n",
      "epoch 32 | loss: 0.6675  | val_0_logloss: 0.66951 |  0:00:39s\n",
      "epoch 33 | loss: 0.66666 | val_0_logloss: 0.66888 |  0:00:41s\n",
      "epoch 34 | loss: 0.66749 | val_0_logloss: 0.66479 |  0:00:42s\n",
      "epoch 35 | loss: 0.66866 | val_0_logloss: 0.66461 |  0:00:43s\n",
      "epoch 36 | loss: 0.66735 | val_0_logloss: 0.66952 |  0:00:44s\n",
      "epoch 37 | loss: 0.66711 | val_0_logloss: 0.66772 |  0:00:45s\n",
      "epoch 38 | loss: 0.66685 | val_0_logloss: 0.66659 |  0:00:46s\n",
      "epoch 39 | loss: 0.66639 | val_0_logloss: 0.67095 |  0:00:48s\n",
      "epoch 40 | loss: 0.66655 | val_0_logloss: 0.66907 |  0:00:49s\n",
      "epoch 41 | loss: 0.66726 | val_0_logloss: 0.66578 |  0:00:50s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_logloss = 0.6634\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 3 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.76004 | val_0_logloss: 0.82903 |  0:00:01s\n",
      "epoch 1  | loss: 0.68144 | val_0_logloss: 0.797   |  0:00:02s\n",
      "epoch 2  | loss: 0.67917 | val_0_logloss: 0.79824 |  0:00:03s\n",
      "epoch 3  | loss: 0.67516 | val_0_logloss: 0.76322 |  0:00:05s\n",
      "epoch 4  | loss: 0.67364 | val_0_logloss: 0.75428 |  0:00:07s\n",
      "epoch 5  | loss: 0.6739  | val_0_logloss: 0.74777 |  0:00:08s\n",
      "epoch 6  | loss: 0.67311 | val_0_logloss: 0.73303 |  0:00:09s\n",
      "epoch 7  | loss: 0.67155 | val_0_logloss: 0.73166 |  0:00:10s\n",
      "epoch 8  | loss: 0.67173 | val_0_logloss: 0.72134 |  0:00:12s\n",
      "epoch 9  | loss: 0.67069 | val_0_logloss: 0.7215  |  0:00:13s\n",
      "epoch 10 | loss: 0.66975 | val_0_logloss: 0.70651 |  0:00:14s\n",
      "epoch 11 | loss: 0.66918 | val_0_logloss: 0.70604 |  0:00:16s\n",
      "epoch 12 | loss: 0.67043 | val_0_logloss: 0.70369 |  0:00:17s\n",
      "epoch 13 | loss: 0.66996 | val_0_logloss: 0.69185 |  0:00:19s\n",
      "epoch 14 | loss: 0.66995 | val_0_logloss: 0.69301 |  0:00:20s\n",
      "epoch 15 | loss: 0.66881 | val_0_logloss: 0.6879  |  0:00:22s\n",
      "epoch 16 | loss: 0.66859 | val_0_logloss: 0.68336 |  0:00:23s\n",
      "epoch 17 | loss: 0.66869 | val_0_logloss: 0.68212 |  0:00:24s\n",
      "epoch 18 | loss: 0.66957 | val_0_logloss: 0.68537 |  0:00:26s\n",
      "epoch 19 | loss: 0.67302 | val_0_logloss: 0.68697 |  0:00:27s\n",
      "epoch 20 | loss: 0.66908 | val_0_logloss: 0.67828 |  0:00:29s\n",
      "epoch 21 | loss: 0.66984 | val_0_logloss: 0.68131 |  0:00:30s\n",
      "epoch 22 | loss: 0.66943 | val_0_logloss: 0.68024 |  0:00:32s\n",
      "epoch 23 | loss: 0.66766 | val_0_logloss: 0.68055 |  0:00:33s\n",
      "epoch 24 | loss: 0.66601 | val_0_logloss: 0.6801  |  0:00:35s\n",
      "epoch 25 | loss: 0.66552 | val_0_logloss: 0.68332 |  0:00:36s\n",
      "epoch 26 | loss: 0.66743 | val_0_logloss: 0.68132 |  0:00:38s\n",
      "epoch 27 | loss: 0.66574 | val_0_logloss: 0.67992 |  0:00:39s\n",
      "epoch 28 | loss: 0.66848 | val_0_logloss: 0.6809  |  0:00:41s\n",
      "epoch 29 | loss: 0.66512 | val_0_logloss: 0.68137 |  0:00:42s\n",
      "epoch 30 | loss: 0.66375 | val_0_logloss: 0.6841  |  0:00:43s\n",
      "\n",
      "Early stopping occurred at epoch 30 with best_epoch = 20 and best_val_0_logloss = 0.67828\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 4 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.76234 | val_0_logloss: 0.80589 |  0:00:00s\n",
      "epoch 1  | loss: 0.68486 | val_0_logloss: 0.7979  |  0:00:01s\n",
      "epoch 2  | loss: 0.68019 | val_0_logloss: 0.77595 |  0:00:02s\n",
      "epoch 3  | loss: 0.67882 | val_0_logloss: 0.75502 |  0:00:03s\n",
      "epoch 4  | loss: 0.67733 | val_0_logloss: 0.74126 |  0:00:04s\n",
      "epoch 5  | loss: 0.68081 | val_0_logloss: 0.73469 |  0:00:05s\n",
      "epoch 6  | loss: 0.67926 | val_0_logloss: 0.72587 |  0:00:06s\n",
      "epoch 7  | loss: 0.67522 | val_0_logloss: 0.71864 |  0:00:07s\n",
      "epoch 8  | loss: 0.67376 | val_0_logloss: 0.71624 |  0:00:08s\n",
      "epoch 9  | loss: 0.67416 | val_0_logloss: 0.70773 |  0:00:09s\n",
      "epoch 10 | loss: 0.67364 | val_0_logloss: 0.7036  |  0:00:10s\n",
      "epoch 11 | loss: 0.67461 | val_0_logloss: 0.70172 |  0:00:11s\n",
      "epoch 12 | loss: 0.67316 | val_0_logloss: 0.69284 |  0:00:12s\n",
      "epoch 13 | loss: 0.67133 | val_0_logloss: 0.68339 |  0:00:13s\n",
      "epoch 14 | loss: 0.67212 | val_0_logloss: 0.68501 |  0:00:14s\n",
      "epoch 15 | loss: 0.6718  | val_0_logloss: 0.67362 |  0:00:15s\n",
      "epoch 16 | loss: 0.67025 | val_0_logloss: 0.67421 |  0:00:16s\n",
      "epoch 17 | loss: 0.67045 | val_0_logloss: 0.67373 |  0:00:17s\n",
      "epoch 18 | loss: 0.67302 | val_0_logloss: 0.68186 |  0:00:18s\n",
      "epoch 19 | loss: 0.67067 | val_0_logloss: 0.67015 |  0:00:18s\n",
      "epoch 20 | loss: 0.67015 | val_0_logloss: 0.67261 |  0:00:19s\n",
      "epoch 21 | loss: 0.67026 | val_0_logloss: 0.67038 |  0:00:20s\n",
      "epoch 22 | loss: 0.66966 | val_0_logloss: 0.67219 |  0:00:21s\n",
      "epoch 23 | loss: 0.66897 | val_0_logloss: 0.67258 |  0:00:22s\n",
      "epoch 24 | loss: 0.67071 | val_0_logloss: 0.66794 |  0:00:23s\n",
      "epoch 25 | loss: 0.67005 | val_0_logloss: 0.67428 |  0:00:24s\n",
      "epoch 26 | loss: 0.66926 | val_0_logloss: 0.66679 |  0:00:25s\n",
      "epoch 27 | loss: 0.66841 | val_0_logloss: 0.66662 |  0:00:26s\n",
      "epoch 28 | loss: 0.66842 | val_0_logloss: 0.6671  |  0:00:27s\n",
      "epoch 29 | loss: 0.66797 | val_0_logloss: 0.66667 |  0:00:28s\n",
      "epoch 30 | loss: 0.66962 | val_0_logloss: 0.6669  |  0:00:29s\n",
      "epoch 31 | loss: 0.66672 | val_0_logloss: 0.67153 |  0:00:30s\n",
      "epoch 32 | loss: 0.6687  | val_0_logloss: 0.66886 |  0:00:31s\n",
      "epoch 33 | loss: 0.66679 | val_0_logloss: 0.66989 |  0:00:32s\n",
      "epoch 34 | loss: 0.66717 | val_0_logloss: 0.67193 |  0:00:33s\n",
      "epoch 35 | loss: 0.66713 | val_0_logloss: 0.66821 |  0:00:34s\n",
      "epoch 36 | loss: 0.66838 | val_0_logloss: 0.66799 |  0:00:35s\n",
      "epoch 37 | loss: 0.66747 | val_0_logloss: 0.67437 |  0:00:36s\n",
      "\n",
      "Early stopping occurred at epoch 37 with best_epoch = 27 and best_val_0_logloss = 0.66662\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 5 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.75471 | val_0_logloss: 0.80256 |  0:00:00s\n",
      "epoch 1  | loss: 0.68318 | val_0_logloss: 0.80937 |  0:00:02s\n",
      "epoch 2  | loss: 0.677   | val_0_logloss: 0.79899 |  0:00:03s\n",
      "epoch 3  | loss: 0.67988 | val_0_logloss: 0.78899 |  0:00:03s\n",
      "epoch 4  | loss: 0.67516 | val_0_logloss: 0.77782 |  0:00:04s\n",
      "epoch 5  | loss: 0.67127 | val_0_logloss: 0.77647 |  0:00:05s\n",
      "epoch 6  | loss: 0.67635 | val_0_logloss: 0.76761 |  0:00:06s\n",
      "epoch 7  | loss: 0.67478 | val_0_logloss: 0.74246 |  0:00:07s\n",
      "epoch 8  | loss: 0.67342 | val_0_logloss: 0.73709 |  0:00:08s\n",
      "epoch 9  | loss: 0.66931 | val_0_logloss: 0.73112 |  0:00:09s\n",
      "epoch 10 | loss: 0.66856 | val_0_logloss: 0.72667 |  0:00:10s\n",
      "epoch 11 | loss: 0.6706  | val_0_logloss: 0.71416 |  0:00:11s\n",
      "epoch 12 | loss: 0.67071 | val_0_logloss: 0.71293 |  0:00:12s\n",
      "epoch 13 | loss: 0.67033 | val_0_logloss: 0.71088 |  0:00:13s\n",
      "epoch 14 | loss: 0.67197 | val_0_logloss: 0.70293 |  0:00:14s\n",
      "epoch 15 | loss: 0.67062 | val_0_logloss: 0.69723 |  0:00:15s\n",
      "epoch 16 | loss: 0.67052 | val_0_logloss: 0.69896 |  0:00:16s\n",
      "epoch 17 | loss: 0.67058 | val_0_logloss: 0.69131 |  0:00:16s\n",
      "epoch 18 | loss: 0.66908 | val_0_logloss: 0.69194 |  0:00:17s\n",
      "epoch 19 | loss: 0.6679  | val_0_logloss: 0.69613 |  0:00:18s\n",
      "epoch 20 | loss: 0.67029 | val_0_logloss: 0.69161 |  0:00:19s\n",
      "epoch 21 | loss: 0.66782 | val_0_logloss: 0.69123 |  0:00:21s\n",
      "epoch 22 | loss: 0.66892 | val_0_logloss: 0.69602 |  0:00:21s\n",
      "epoch 23 | loss: 0.66847 | val_0_logloss: 0.69128 |  0:00:22s\n",
      "epoch 24 | loss: 0.66783 | val_0_logloss: 0.69044 |  0:00:24s\n",
      "epoch 25 | loss: 0.66728 | val_0_logloss: 0.69052 |  0:00:24s\n",
      "epoch 26 | loss: 0.66757 | val_0_logloss: 0.69064 |  0:00:25s\n",
      "epoch 27 | loss: 0.669   | val_0_logloss: 0.69331 |  0:00:26s\n",
      "epoch 28 | loss: 0.66726 | val_0_logloss: 0.69421 |  0:00:27s\n",
      "epoch 29 | loss: 0.6666  | val_0_logloss: 0.69405 |  0:00:28s\n",
      "epoch 30 | loss: 0.66624 | val_0_logloss: 0.70073 |  0:00:29s\n",
      "epoch 31 | loss: 0.66678 | val_0_logloss: 0.69052 |  0:00:30s\n",
      "epoch 32 | loss: 0.66617 | val_0_logloss: 0.69607 |  0:00:31s\n",
      "epoch 33 | loss: 0.66817 | val_0_logloss: 0.68809 |  0:00:32s\n",
      "epoch 34 | loss: 0.66635 | val_0_logloss: 0.68927 |  0:00:33s\n",
      "epoch 35 | loss: 0.66459 | val_0_logloss: 0.695   |  0:00:34s\n",
      "epoch 36 | loss: 0.66384 | val_0_logloss: 0.69086 |  0:00:35s\n",
      "epoch 37 | loss: 0.66617 | val_0_logloss: 0.68752 |  0:00:36s\n",
      "epoch 38 | loss: 0.66503 | val_0_logloss: 0.69065 |  0:00:37s\n",
      "epoch 39 | loss: 0.66625 | val_0_logloss: 0.68825 |  0:00:38s\n",
      "epoch 40 | loss: 0.66579 | val_0_logloss: 0.68913 |  0:00:38s\n",
      "epoch 41 | loss: 0.66381 | val_0_logloss: 0.6939  |  0:00:39s\n",
      "epoch 42 | loss: 0.66543 | val_0_logloss: 0.68626 |  0:00:40s\n",
      "epoch 43 | loss: 0.66473 | val_0_logloss: 0.68971 |  0:00:41s\n",
      "epoch 44 | loss: 0.66318 | val_0_logloss: 0.69015 |  0:00:42s\n",
      "epoch 45 | loss: 0.66359 | val_0_logloss: 0.68946 |  0:00:43s\n",
      "epoch 46 | loss: 0.66379 | val_0_logloss: 0.69048 |  0:00:44s\n",
      "epoch 47 | loss: 0.663   | val_0_logloss: 0.69363 |  0:00:45s\n",
      "epoch 48 | loss: 0.66311 | val_0_logloss: 0.68813 |  0:00:46s\n",
      "epoch 49 | loss: 0.66358 | val_0_logloss: 0.68865 |  0:00:47s\n",
      "epoch 50 | loss: 0.66322 | val_0_logloss: 0.69161 |  0:00:48s\n",
      "epoch 51 | loss: 0.66346 | val_0_logloss: 0.69665 |  0:00:49s\n",
      "epoch 52 | loss: 0.66255 | val_0_logloss: 0.6915  |  0:00:50s\n",
      "\n",
      "Early stopping occurred at epoch 52 with best_epoch = 42 and best_val_0_logloss = 0.68626\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 6 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.75249 | val_0_logloss: 0.7806  |  0:00:00s\n",
      "epoch 1  | loss: 0.68507 | val_0_logloss: 0.81164 |  0:00:01s\n",
      "epoch 2  | loss: 0.68232 | val_0_logloss: 0.80742 |  0:00:03s\n",
      "epoch 3  | loss: 0.68007 | val_0_logloss: 0.78865 |  0:00:04s\n",
      "epoch 4  | loss: 0.6768  | val_0_logloss: 0.79321 |  0:00:05s\n",
      "epoch 5  | loss: 0.6767  | val_0_logloss: 0.77771 |  0:00:06s\n",
      "epoch 6  | loss: 0.67572 | val_0_logloss: 0.7457  |  0:00:07s\n",
      "epoch 7  | loss: 0.6743  | val_0_logloss: 0.75094 |  0:00:08s\n",
      "epoch 8  | loss: 0.67251 | val_0_logloss: 0.73068 |  0:00:09s\n",
      "epoch 9  | loss: 0.67301 | val_0_logloss: 0.72269 |  0:00:10s\n",
      "epoch 10 | loss: 0.67287 | val_0_logloss: 0.72003 |  0:00:10s\n",
      "epoch 11 | loss: 0.67096 | val_0_logloss: 0.71264 |  0:00:11s\n",
      "epoch 12 | loss: 0.67454 | val_0_logloss: 0.70264 |  0:00:12s\n",
      "epoch 13 | loss: 0.67325 | val_0_logloss: 0.69038 |  0:00:13s\n",
      "epoch 14 | loss: 0.67153 | val_0_logloss: 0.68331 |  0:00:14s\n",
      "epoch 15 | loss: 0.67212 | val_0_logloss: 0.68339 |  0:00:15s\n",
      "epoch 16 | loss: 0.67097 | val_0_logloss: 0.68864 |  0:00:16s\n",
      "epoch 17 | loss: 0.67088 | val_0_logloss: 0.67159 |  0:00:17s\n",
      "epoch 18 | loss: 0.67018 | val_0_logloss: 0.68536 |  0:00:18s\n",
      "epoch 19 | loss: 0.66951 | val_0_logloss: 0.66964 |  0:00:19s\n",
      "epoch 20 | loss: 0.67035 | val_0_logloss: 0.67681 |  0:00:20s\n",
      "epoch 21 | loss: 0.67151 | val_0_logloss: 0.67296 |  0:00:21s\n",
      "epoch 22 | loss: 0.66997 | val_0_logloss: 0.69876 |  0:00:22s\n",
      "epoch 23 | loss: 0.6706  | val_0_logloss: 0.67367 |  0:00:23s\n",
      "epoch 24 | loss: 0.67011 | val_0_logloss: 0.67523 |  0:00:23s\n",
      "epoch 25 | loss: 0.67    | val_0_logloss: 0.67487 |  0:00:24s\n",
      "epoch 26 | loss: 0.66948 | val_0_logloss: 0.67494 |  0:00:25s\n",
      "epoch 27 | loss: 0.66818 | val_0_logloss: 0.67419 |  0:00:26s\n",
      "epoch 28 | loss: 0.66913 | val_0_logloss: 0.67485 |  0:00:27s\n",
      "epoch 29 | loss: 0.66903 | val_0_logloss: 0.67961 |  0:00:28s\n",
      "\n",
      "Early stopping occurred at epoch 29 with best_epoch = 19 and best_val_0_logloss = 0.66964\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 7 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.75242 | val_0_logloss: 0.79385 |  0:00:00s\n",
      "epoch 1  | loss: 0.68865 | val_0_logloss: 0.77568 |  0:00:01s\n",
      "epoch 2  | loss: 0.67939 | val_0_logloss: 0.77554 |  0:00:02s\n",
      "epoch 3  | loss: 0.67858 | val_0_logloss: 0.76159 |  0:00:03s\n",
      "epoch 4  | loss: 0.67404 | val_0_logloss: 0.76895 |  0:00:04s\n",
      "epoch 5  | loss: 0.67426 | val_0_logloss: 0.74723 |  0:00:05s\n",
      "epoch 6  | loss: 0.67395 | val_0_logloss: 0.74427 |  0:00:06s\n",
      "epoch 7  | loss: 0.67456 | val_0_logloss: 0.72171 |  0:00:07s\n",
      "epoch 8  | loss: 0.67303 | val_0_logloss: 0.72393 |  0:00:08s\n",
      "epoch 9  | loss: 0.67176 | val_0_logloss: 0.71519 |  0:00:09s\n",
      "epoch 10 | loss: 0.66969 | val_0_logloss: 0.71304 |  0:00:10s\n",
      "epoch 11 | loss: 0.67158 | val_0_logloss: 0.70897 |  0:00:11s\n",
      "epoch 12 | loss: 0.67033 | val_0_logloss: 0.70307 |  0:00:12s\n",
      "epoch 13 | loss: 0.67036 | val_0_logloss: 0.6902  |  0:00:13s\n",
      "epoch 14 | loss: 0.67129 | val_0_logloss: 0.68616 |  0:00:14s\n",
      "epoch 15 | loss: 0.6698  | val_0_logloss: 0.67964 |  0:00:15s\n",
      "epoch 16 | loss: 0.67055 | val_0_logloss: 0.685   |  0:00:16s\n",
      "epoch 17 | loss: 0.67155 | val_0_logloss: 0.68746 |  0:00:17s\n",
      "epoch 18 | loss: 0.66819 | val_0_logloss: 0.67614 |  0:00:18s\n",
      "epoch 19 | loss: 0.66941 | val_0_logloss: 0.67789 |  0:00:18s\n",
      "epoch 20 | loss: 0.66973 | val_0_logloss: 0.68102 |  0:00:20s\n",
      "epoch 21 | loss: 0.66953 | val_0_logloss: 0.67667 |  0:00:21s\n",
      "epoch 22 | loss: 0.66909 | val_0_logloss: 0.6794  |  0:00:21s\n",
      "epoch 23 | loss: 0.67077 | val_0_logloss: 0.68348 |  0:00:22s\n",
      "epoch 24 | loss: 0.66865 | val_0_logloss: 0.67516 |  0:00:23s\n",
      "epoch 25 | loss: 0.66766 | val_0_logloss: 0.67386 |  0:00:24s\n",
      "epoch 26 | loss: 0.66867 | val_0_logloss: 0.67519 |  0:00:25s\n",
      "epoch 27 | loss: 0.66714 | val_0_logloss: 0.68461 |  0:00:26s\n",
      "epoch 28 | loss: 0.66817 | val_0_logloss: 0.68027 |  0:00:27s\n",
      "epoch 29 | loss: 0.66902 | val_0_logloss: 0.67629 |  0:00:28s\n",
      "epoch 30 | loss: 0.66634 | val_0_logloss: 0.67468 |  0:00:29s\n",
      "epoch 31 | loss: 0.66813 | val_0_logloss: 0.68112 |  0:00:30s\n",
      "epoch 32 | loss: 0.66851 | val_0_logloss: 0.68093 |  0:00:31s\n",
      "epoch 33 | loss: 0.6668  | val_0_logloss: 0.67535 |  0:00:32s\n",
      "epoch 34 | loss: 0.66563 | val_0_logloss: 0.67731 |  0:00:33s\n",
      "epoch 35 | loss: 0.66785 | val_0_logloss: 0.67708 |  0:00:34s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_logloss = 0.67386\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 8 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.75063 | val_0_logloss: 0.79873 |  0:00:00s\n",
      "epoch 1  | loss: 0.68406 | val_0_logloss: 0.79234 |  0:00:01s\n",
      "epoch 2  | loss: 0.67943 | val_0_logloss: 0.77788 |  0:00:02s\n",
      "epoch 3  | loss: 0.67565 | val_0_logloss: 0.77281 |  0:00:03s\n",
      "epoch 4  | loss: 0.67845 | val_0_logloss: 0.75539 |  0:00:04s\n",
      "epoch 5  | loss: 0.67766 | val_0_logloss: 0.75673 |  0:00:06s\n",
      "epoch 6  | loss: 0.67378 | val_0_logloss: 0.74201 |  0:00:06s\n",
      "epoch 7  | loss: 0.67431 | val_0_logloss: 0.73752 |  0:00:08s\n",
      "epoch 8  | loss: 0.67337 | val_0_logloss: 0.73    |  0:00:08s\n",
      "epoch 9  | loss: 0.67481 | val_0_logloss: 0.7165  |  0:00:09s\n",
      "epoch 10 | loss: 0.67137 | val_0_logloss: 0.7106  |  0:00:10s\n",
      "epoch 11 | loss: 0.67237 | val_0_logloss: 0.71089 |  0:00:11s\n",
      "epoch 12 | loss: 0.67309 | val_0_logloss: 0.69864 |  0:00:12s\n",
      "epoch 13 | loss: 0.67148 | val_0_logloss: 0.70366 |  0:00:13s\n",
      "epoch 14 | loss: 0.67186 | val_0_logloss: 0.6932  |  0:00:14s\n",
      "epoch 15 | loss: 0.67163 | val_0_logloss: 0.69077 |  0:00:15s\n",
      "epoch 16 | loss: 0.67169 | val_0_logloss: 0.69531 |  0:00:16s\n",
      "epoch 17 | loss: 0.67254 | val_0_logloss: 0.68191 |  0:00:17s\n",
      "epoch 18 | loss: 0.67119 | val_0_logloss: 0.67957 |  0:00:18s\n",
      "epoch 19 | loss: 0.67071 | val_0_logloss: 0.67638 |  0:00:19s\n",
      "epoch 20 | loss: 0.67236 | val_0_logloss: 0.67679 |  0:00:20s\n",
      "epoch 21 | loss: 0.67056 | val_0_logloss: 0.67841 |  0:00:21s\n",
      "epoch 22 | loss: 0.67002 | val_0_logloss: 0.67787 |  0:00:22s\n",
      "epoch 23 | loss: 0.6694  | val_0_logloss: 0.67345 |  0:00:22s\n",
      "epoch 24 | loss: 0.66732 | val_0_logloss: 0.67618 |  0:00:23s\n",
      "epoch 25 | loss: 0.67081 | val_0_logloss: 0.6743  |  0:00:24s\n",
      "epoch 26 | loss: 0.6712  | val_0_logloss: 0.67979 |  0:00:25s\n",
      "epoch 27 | loss: 0.66737 | val_0_logloss: 0.6838  |  0:00:26s\n",
      "epoch 28 | loss: 0.66911 | val_0_logloss: 0.67564 |  0:00:27s\n",
      "epoch 29 | loss: 0.66811 | val_0_logloss: 0.67745 |  0:00:28s\n",
      "epoch 30 | loss: 0.66607 | val_0_logloss: 0.67708 |  0:00:29s\n",
      "epoch 31 | loss: 0.66867 | val_0_logloss: 0.67637 |  0:00:30s\n",
      "epoch 32 | loss: 0.6678  | val_0_logloss: 0.67733 |  0:00:31s\n",
      "epoch 33 | loss: 0.66739 | val_0_logloss: 0.67934 |  0:00:32s\n",
      "\n",
      "Early stopping occurred at epoch 33 with best_epoch = 23 and best_val_0_logloss = 0.67345\n",
      "Best weights from best epoch are automatically used!\n",
      "============ Fold 9 ============\n",
      "\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 0.7427  | val_0_logloss: 0.80883 |  0:00:00s\n",
      "epoch 1  | loss: 0.68237 | val_0_logloss: 0.77888 |  0:00:01s\n",
      "epoch 2  | loss: 0.6817  | val_0_logloss: 0.7642  |  0:00:02s\n",
      "epoch 3  | loss: 0.67734 | val_0_logloss: 0.76659 |  0:00:03s\n",
      "epoch 4  | loss: 0.67485 | val_0_logloss: 0.76513 |  0:00:05s\n",
      "epoch 5  | loss: 0.67852 | val_0_logloss: 0.74085 |  0:00:05s\n",
      "epoch 6  | loss: 0.67776 | val_0_logloss: 0.75434 |  0:00:07s\n",
      "epoch 7  | loss: 0.6729  | val_0_logloss: 0.73803 |  0:00:08s\n",
      "epoch 8  | loss: 0.6739  | val_0_logloss: 0.72908 |  0:00:08s\n",
      "epoch 9  | loss: 0.67346 | val_0_logloss: 0.70952 |  0:00:09s\n",
      "epoch 10 | loss: 0.67192 | val_0_logloss: 0.71826 |  0:00:10s\n",
      "epoch 11 | loss: 0.67151 | val_0_logloss: 0.70568 |  0:00:11s\n",
      "epoch 12 | loss: 0.67138 | val_0_logloss: 0.70277 |  0:00:12s\n",
      "epoch 13 | loss: 0.67232 | val_0_logloss: 0.69886 |  0:00:13s\n",
      "epoch 14 | loss: 0.67268 | val_0_logloss: 0.69689 |  0:00:14s\n",
      "epoch 15 | loss: 0.67246 | val_0_logloss: 0.68731 |  0:00:15s\n",
      "epoch 16 | loss: 0.66953 | val_0_logloss: 0.68503 |  0:00:16s\n",
      "epoch 17 | loss: 0.66987 | val_0_logloss: 0.68124 |  0:00:17s\n",
      "epoch 18 | loss: 0.6719  | val_0_logloss: 0.68172 |  0:00:18s\n",
      "epoch 19 | loss: 0.67004 | val_0_logloss: 0.68126 |  0:00:19s\n",
      "epoch 20 | loss: 0.66967 | val_0_logloss: 0.67884 |  0:00:20s\n",
      "epoch 21 | loss: 0.67229 | val_0_logloss: 0.68112 |  0:00:21s\n",
      "epoch 22 | loss: 0.67083 | val_0_logloss: 0.67929 |  0:00:22s\n",
      "epoch 23 | loss: 0.66994 | val_0_logloss: 0.67939 |  0:00:22s\n",
      "epoch 24 | loss: 0.66893 | val_0_logloss: 0.67922 |  0:00:23s\n",
      "epoch 25 | loss: 0.66646 | val_0_logloss: 0.6792  |  0:00:24s\n",
      "epoch 26 | loss: 0.66861 | val_0_logloss: 0.67912 |  0:00:25s\n",
      "epoch 27 | loss: 0.66736 | val_0_logloss: 0.6803  |  0:00:26s\n",
      "epoch 28 | loss: 0.66613 | val_0_logloss: 0.67689 |  0:00:27s\n",
      "epoch 29 | loss: 0.66835 | val_0_logloss: 0.67682 |  0:00:28s\n",
      "epoch 30 | loss: 0.66656 | val_0_logloss: 0.67592 |  0:00:29s\n",
      "epoch 31 | loss: 0.66774 | val_0_logloss: 0.67765 |  0:00:30s\n",
      "epoch 32 | loss: 0.66817 | val_0_logloss: 0.68074 |  0:00:31s\n",
      "epoch 33 | loss: 0.66702 | val_0_logloss: 0.67596 |  0:00:32s\n",
      "epoch 34 | loss: 0.66663 | val_0_logloss: 0.68378 |  0:00:33s\n",
      "epoch 35 | loss: 0.66583 | val_0_logloss: 0.67825 |  0:00:34s\n",
      "epoch 36 | loss: 0.6674  | val_0_logloss: 0.67647 |  0:00:35s\n",
      "epoch 37 | loss: 0.66583 | val_0_logloss: 0.67801 |  0:00:36s\n",
      "epoch 38 | loss: 0.66603 | val_0_logloss: 0.67776 |  0:00:36s\n",
      "epoch 39 | loss: 0.66502 | val_0_logloss: 0.67785 |  0:00:37s\n",
      "epoch 40 | loss: 0.66506 | val_0_logloss: 0.68004 |  0:00:39s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_logloss = 0.67592\n",
      "Best weights from best epoch are automatically used!\n",
      "Log Loss Score: 0.67069\n"
     ]
    }
   ],
   "source": [
    "n_fold = 10\n",
    "folds = StratifiedKFold(n_splits=n_fold, shuffle=True, random_state=42)\n",
    "splits = folds.split(train_pred, train_y)\n",
    "net_oof = np.zeros((train_pred.shape[0], 3))\n",
    "net_preds = np.zeros((test_pred.shape[0], 3))\n",
    "for fold, (train_idx, valid_idx) in enumerate(splits):\n",
    "    print(f\"============ Fold {fold} ============\\n\")\n",
    "    X_train, X_valid = train_pred[train_idx], train_pred[valid_idx]\n",
    "    y_train, y_valid = train_y[train_idx], train_y[valid_idx]\n",
    "    model = TabNetMultiTaskClassifier(\n",
    "            n_d=64, n_a=64, n_steps=1,\n",
    "            lambda_sparse=1e-4,\n",
    "            optimizer_fn=torch.optim.Adam,\n",
    "            optimizer_params=dict(lr=2e-2),\n",
    "            scheduler_params = {\"gamma\": 0.9, \"step_size\": 50},\n",
    "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "            mask_type=\"entmax\", \n",
    "            device_name=device\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train.reshape(-1,1),\n",
    "        eval_set=[(X_valid, y_valid.reshape(-1,1))],\n",
    "        max_epochs=100,\n",
    "        batch_size=1024,\n",
    "        eval_metric=[\"logloss\"],\n",
    "        virtual_batch_size=128,\n",
    "        num_workers=1,\n",
    "        drop_last=False\n",
    "    )\n",
    "    net_oof[valid_idx] = model.predict_proba(X_valid)\n",
    "    net_preds += model.predict_proba(test_pred)[0] / n_fold\n",
    "log_score = log_loss(train_y, net_oof)\n",
    "print(f\"Log Loss Score: {log_score:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-05-24T04:59:40.595249Z",
     "iopub.status.busy": "2021-05-24T04:59:40.594637Z",
     "iopub.status.idle": "2021-05-24T04:59:40.737394Z",
     "shell.execute_reply": "2021-05-24T04:59:40.736550Z",
     "shell.execute_reply.started": "2021-05-24T04:59:40.595204Z"
    }
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv(\"/kaggle/input/predictcreditcarddelinquency/sample_submission.csv\")\n",
    "submission.iloc[:, 1:] = net_preds\n",
    "submission.to_csv(\"meta_ensemble_submit.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
