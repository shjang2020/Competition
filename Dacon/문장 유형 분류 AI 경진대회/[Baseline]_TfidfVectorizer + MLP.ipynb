{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "CFG = {\n",
    "    'EPOCHS':5,\n",
    "    'LEARNING_RATE':1e-4,\n",
    "    'BATCH_SIZE':32,\n",
    "    'SEED':41\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed RandomSeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(CFG['SEED']) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/train.csv')\n",
    "test = pd.read_csv('data/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train / Validation Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제공된 학습데이터를 학습 / 검증 데이터셋으로 재 분할\n",
    "train, val, _, _ = train_test_split(df, df['label'], test_size=0.2, random_state=CFG['SEED'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Pre-processing\n",
    "### 1. 문장(Text) 벡터화\n",
    "### 2. Label Encoding (유형, 극성, 시제, 확실성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13232, 9351) (3309, 9351) (7090, 9351)\n"
     ]
    }
   ],
   "source": [
    "# 1. 문장(Text) 벡터화 -> TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(min_df = 4, analyzer = 'word', ngram_range=(1, 2))\n",
    "vectorizer.fit(np.array(train[\"문장\"]))\n",
    "\n",
    "train_vec = vectorizer.transform(train[\"문장\"])\n",
    "val_vec = vectorizer.transform(val[\"문장\"])\n",
    "test_vec = vectorizer.transform(test[\"문장\"])\n",
    "\n",
    "print(train_vec.shape, val_vec.shape, test_vec.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Label Encoding (유형, 극성, 시제, 확실성)\n",
    "type_le = preprocessing.LabelEncoder()\n",
    "train[\"유형\"] = type_le.fit_transform(train[\"유형\"].values)\n",
    "val[\"유형\"] = type_le.transform(val[\"유형\"].values)\n",
    "\n",
    "polarity_le = preprocessing.LabelEncoder()\n",
    "train[\"극성\"] = polarity_le.fit_transform(train[\"극성\"].values)\n",
    "val[\"극성\"] = polarity_le.transform(val[\"극성\"].values)\n",
    "\n",
    "tense_le = preprocessing.LabelEncoder()\n",
    "train[\"시제\"] = tense_le.fit_transform(train[\"시제\"].values)\n",
    "val[\"시제\"] = tense_le.transform(val[\"시제\"].values)\n",
    "\n",
    "certainty_le = preprocessing.LabelEncoder()\n",
    "train[\"확실성\"] = certainty_le.fit_transform(train[\"확실성\"].values)\n",
    "val[\"확실성\"] = certainty_le.transform(val[\"확실성\"].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type = train[\"유형\"].values # sentence type\n",
    "train_polarity = train[\"극성\"].values # sentence polarity\n",
    "train_tense = train[\"시제\"].values # sentence tense\n",
    "train_certainty = train[\"확실성\"].values # sentence certainty\n",
    "\n",
    "train_labels = {\n",
    "    'type' : train_type,\n",
    "    'polarity' : train_polarity,\n",
    "    'tense' : train_tense,\n",
    "    'certainty' : train_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_type = val[\"유형\"].values # sentence type\n",
    "val_polarity = val[\"극성\"].values # sentence polarity\n",
    "val_tense = val[\"시제\"].values # sentence tense\n",
    "val_certainty = val[\"확실성\"].values # sentence certainty\n",
    "\n",
    "val_labels = {\n",
    "    'type' : val_type,\n",
    "    'polarity' : val_polarity,\n",
    "    'tense' : val_tense,\n",
    "    'certainty' : val_certainty\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CustomDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, st_vec, st_labels):\n",
    "        self.st_vec = st_vec\n",
    "        self.st_labels = st_labels\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        st_vector = torch.FloatTensor(self.st_vec[index].toarray()).squeeze(0)\n",
    "        if self.st_labels is not None:\n",
    "            st_type = self.st_labels['type'][index]\n",
    "            st_polarity = self.st_labels['polarity'][index]\n",
    "            st_tense = self.st_labels['tense'][index]\n",
    "            st_certainty = self.st_labels['certainty'][index]\n",
    "            return st_vector, st_type, st_polarity, st_tense, st_certainty\n",
    "        else:\n",
    "            return st_vector\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.st_vec.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CustomDataset(train_vec, train_labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=True, num_workers=0)\n",
    "\n",
    "val_dataset = CustomDataset(val_vec, val_labels)\n",
    "val_loader = DataLoader(val_dataset, batch_size = CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Define"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(nn.Module):\n",
    "    def __init__(self, input_dim=9351):\n",
    "        super(BaseModel, self).__init__()\n",
    "        self.feature_extract = nn.Sequential(\n",
    "            nn.Linear(in_features=input_dim, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=1024),\n",
    "            nn.BatchNorm1d(1024),\n",
    "            nn.LeakyReLU(),\n",
    "            nn.Linear(in_features=1024, out_features=512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.LeakyReLU(),\n",
    "        )\n",
    "        self.type_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=4),\n",
    "        )\n",
    "        self.polarity_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3),\n",
    "        )\n",
    "        self.tense_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=3),\n",
    "        )\n",
    "        self.certainty_classifier = nn.Sequential(\n",
    "            nn.Dropout(p=0.3),\n",
    "            nn.Linear(in_features=512, out_features=2),\n",
    "        )\n",
    "            \n",
    "    def forward(self, x):\n",
    "        x = self.feature_extract(x)\n",
    "        # 문장 유형, 극성, 시제, 확실성을 각각 분류\n",
    "        type_output = self.type_classifier(x)\n",
    "        polarity_output = self.polarity_classifier(x)\n",
    "        tense_output = self.tense_classifier(x)\n",
    "        certainty_output = self.certainty_classifier(x)\n",
    "        return type_output, polarity_output, tense_output, certainty_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, train_loader, val_loader, scheduler, device):\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = {\n",
    "        'type' : nn.CrossEntropyLoss().to(device),\n",
    "        'polarity' : nn.CrossEntropyLoss().to(device),\n",
    "        'tense' : nn.CrossEntropyLoss().to(device),\n",
    "        'certainty' : nn.CrossEntropyLoss().to(device)\n",
    "    }\n",
    "    \n",
    "    best_loss = 999999\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(1, CFG['EPOCHS']+1):\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for sentence, type_label, polarity_label, tense_label, certainty_label in tqdm(iter(train_loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            loss = 0.25 * criterion['type'](type_logit, type_label) + \\\n",
    "                    0.25 * criterion['polarity'](polarity_logit, polarity_label) + \\\n",
    "                    0.25 * criterion['tense'](tense_logit, tense_label) + \\\n",
    "                    0.25 * criterion['certainty'](certainty_logit, certainty_label)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss.append(loss.item())\n",
    "        \n",
    "        val_loss, val_type_f1, val_polarity_f1, val_tense_f1, val_certainty_f1 = validation(model, val_loader, criterion, device)\n",
    "        print(f'Epoch : [{epoch}] Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] 유형 F1 : [{val_type_f1:.5f}] 극성 F1 : [{val_polarity_f1:.5f}] 시제 F1 : [{val_tense_f1:.5f}] 확실성 F1 : [{val_certainty_f1:.5f}]')\n",
    "        \n",
    "        if scheduler is not None:\n",
    "            scheduler.step(val_loss)\n",
    "            \n",
    "        if best_loss > val_loss:\n",
    "            best_loss = val_loss\n",
    "            best_model = model\n",
    "            \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation(model, val_loader, criterion, device):\n",
    "    model.eval()\n",
    "    val_loss = []\n",
    "    \n",
    "    type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "    type_labels, polarity_labels, tense_labels, certainty_labels = [], [], [], []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence, type_label, polarity_label, tense_label, certainty_label in tqdm(iter(val_loader)):\n",
    "            sentence = sentence.to(device)\n",
    "            type_label = type_label.to(device)\n",
    "            polarity_label = polarity_label.to(device)\n",
    "            tense_label = tense_label.to(device)\n",
    "            certainty_label = certainty_label.to(device)\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            loss = 0.25 * criterion['type'](type_logit, type_label) + \\\n",
    "                    0.25 * criterion['polarity'](polarity_logit, polarity_label) + \\\n",
    "                    0.25 * criterion['tense'](tense_logit, tense_label) + \\\n",
    "                    0.25 * criterion['certainty'](certainty_logit, certainty_label)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            type_preds += type_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            type_labels += type_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            polarity_preds += polarity_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            polarity_labels += polarity_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            tense_preds += tense_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            tense_labels += tense_label.detach().cpu().numpy().tolist()\n",
    "            \n",
    "            certainty_preds += certainty_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            certainty_labels += certainty_label.detach().cpu().numpy().tolist()\n",
    "    \n",
    "    type_f1 = f1_score(type_labels, type_preds, average='weighted')\n",
    "    polarity_f1 = f1_score(polarity_labels, polarity_preds, average='weighted')\n",
    "    tense_f1 = f1_score(tense_labels, tense_preds, average='weighted')\n",
    "    certainty_f1 = f1_score(certainty_labels, certainty_preds, average='weighted')\n",
    "    \n",
    "    return np.mean(val_loss), type_f1, polarity_f1, tense_f1, certainty_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a3afc686a74990a0e8b7cb5b9290ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5752783c03b1436e800d5c7b42921048",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [1] Train Loss : [0.50720] Val Loss : [0.40078] 유형 F1 : [0.77084] 극성 F1 : [0.94569] 시제 F1 : [0.71000] 확실성 F1 : [0.88456]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0166412a98b45f18c703be0de69abc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61f963a255b3473faf3af12eaae08fd4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [2] Train Loss : [0.18007] Val Loss : [0.42119] 유형 F1 : [0.80140] 극성 F1 : [0.95535] 시제 F1 : [0.70352] 확실성 F1 : [0.89303]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc4a27dbb1a4e169e4998545e2ca551",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e88f939dc54d48bcce4854896f90f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [3] Train Loss : [0.07004] Val Loss : [0.46698] 유형 F1 : [0.79205] 극성 F1 : [0.95661] 시제 F1 : [0.71071] 확실성 F1 : [0.88996]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15350fac2bad426e9bee48ddba2646f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fef7fe1a0ed946de93fe8bf6c9b29422",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [4] Train Loss : [0.03858] Val Loss : [0.50601] 유형 F1 : [0.79030] 극성 F1 : [0.95741] 시제 F1 : [0.71217] 확실성 F1 : [0.89087]\n",
      "Epoch     4: reducing learning rate of group 0 to 5.0000e-05.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83c7c709f3564de39a81f7d9b46d3c36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/414 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90a4672430d54b24b02a4f48e66f6b45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/104 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch : [5] Train Loss : [0.02475] Val Loss : [0.52765] 유형 F1 : [0.79512] 극성 F1 : [0.95634] 시제 F1 : [0.70328] 확실성 F1 : [0.89082]\n"
     ]
    }
   ],
   "source": [
    "model = BaseModel()\n",
    "model.eval()\n",
    "optimizer = torch.optim.Adam(params = model.parameters(), lr = CFG[\"LEARNING_RATE\"])\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=2,threshold_mode='abs',min_lr=1e-8, verbose=True)\n",
    "\n",
    "infer_model = train(model, optimizer, train_loader, val_loader, scheduler, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CustomDataset(test_vec, None)\n",
    "test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE'], shuffle=False, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, test_loader, device):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    type_preds, polarity_preds, tense_preds, certainty_preds = [], [], [], []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for sentence in tqdm(test_loader):\n",
    "            sentence = sentence.to(device)\n",
    "            \n",
    "            type_logit, polarity_logit, tense_logit, certainty_logit = model(sentence)\n",
    "            \n",
    "            type_preds += type_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            polarity_preds += polarity_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            tense_preds += tense_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            certainty_preds += certainty_logit.argmax(1).detach().cpu().numpy().tolist()\n",
    "            \n",
    "    return type_preds, polarity_preds, tense_preds, certainty_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8456edaf19f42f5885de4ff3cae3203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/222 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "type_preds, polarity_preds, tense_preds, certainty_preds = inference(model, test_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "type_preds = type_le.inverse_transform(type_preds)\n",
    "polarity_preds = polarity_le.inverse_transform(polarity_preds)\n",
    "tense_preds = tense_le.inverse_transform(tense_preds)\n",
    "certainty_preds = certainty_le.inverse_transform(certainty_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for type_pred, polarity_pred, tense_pred, certainty_pred in zip(type_preds, polarity_preds, tense_preds, certainty_preds):\n",
    "    predictions.append(type_pred+'-'+polarity_pred+'-'+tense_pred+'-'+certainty_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv('data/sample_submission.csv')\n",
    "submit['label'] = predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>TEST_0000</td>\n",
       "      <td>사실형-긍정-현재-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>TEST_0001</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TEST_0002</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>TEST_0003</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>TEST_0004</td>\n",
       "      <td>사실형-긍정-과거-확실</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          ID         label\n",
       "0  TEST_0000  사실형-긍정-현재-확실\n",
       "1  TEST_0001  사실형-긍정-과거-확실\n",
       "2  TEST_0002  사실형-긍정-과거-확실\n",
       "3  TEST_0003  사실형-긍정-과거-확실\n",
       "4  TEST_0004  사실형-긍정-과거-확실"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       사실형-긍정-현재-확실\n",
       "1       사실형-긍정-과거-확실\n",
       "2       사실형-긍정-과거-확실\n",
       "3       사실형-긍정-과거-확실\n",
       "4       사실형-긍정-과거-확실\n",
       "            ...     \n",
       "7085    사실형-긍정-현재-확실\n",
       "7086    추론형-긍정-현재-확실\n",
       "7087    사실형-긍정-현재-확실\n",
       "7088    사실형-긍정-미래-확실\n",
       "7089    사실형-긍정-현재-확실\n",
       "Name: label, Length: 7090, dtype: object"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "submit.label.str[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "submit.to_csv('./baseline_submit.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
