{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"_WnEkFzOHmvF","executionInfo":{"status":"ok","timestamp":1671550466525,"user_tz":-540,"elapsed":15000,"user":{"displayName":"장성현","userId":"03380487854488523631"}},"outputId":"f14a39a5-df0b-49fa-e8d0-ba5d47455153","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[K     |████████████████████████████████| 5.8 MB 18.0 MB/s \n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 76.5 MB/s \n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 51.3 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}],"source":["!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vm2GggmFqR3O","outputId":"cf0ead83-0108-4591-a70d-b1587ffc7b3b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Collecting adabelief-pytorch==0.2.0\n","  Downloading adabelief_pytorch-0.2.0-py3-none-any.whl (5.7 kB)\n","Requirement already satisfied: tabulate>=0.7 in /usr/local/lib/python3.8/dist-packages (from adabelief-pytorch==0.2.0) (0.8.7)\n","Requirement already satisfied: torch>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from adabelief-pytorch==0.2.0) (1.9.0+cu111)\n","Requirement already satisfied: colorama>=0.4.0 in /usr/local/lib/python3.8/dist-packages (from adabelief-pytorch==0.2.0) (0.4.3)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.8/dist-packages (from torch>=0.4.0->adabelief-pytorch==0.2.0) (3.7.4.1)\n","Installing collected packages: adabelief-pytorch\n","Successfully installed adabelief-pytorch-0.2.0\n","\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["!pip install adabelief-pytorch==0.2.0"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xzYDGhkdHmvK","outputId":"8db23466-5d7c-4701-f12f-a5712ed5f4b8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Defaulting to user installation because normal site-packages is not writeable\n","Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n","Requirement already satisfied: nbformat in /usr/local/lib/python3.8/dist-packages (5.0.4)\n","Collecting nbformat\n","  Downloading nbformat-5.7.1-py3-none-any.whl (77 kB)\n","\u001b[K     |████████████████████████████████| 77 kB 14.2 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: jupyter-core in /usr/local/lib/python3.8/dist-packages (from nbformat) (4.6.3)\n","Collecting traitlets>=5.1\n","  Downloading traitlets-5.8.0-py3-none-any.whl (116 kB)\n","\u001b[K     |████████████████████████████████| 116 kB 30.3 MB/s eta 0:00:01\n","\u001b[?25hRequirement already satisfied: jsonschema>=2.6 in /usr/local/lib/python3.8/dist-packages (from nbformat) (3.2.0)\n","Collecting fastjsonschema\n","  Downloading fastjsonschema-2.16.2-py3-none-any.whl (22 kB)\n","Requirement already satisfied: six>=1.11.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (1.15.0)\n","Requirement already satisfied: pyrsistent>=0.14.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (0.15.7)\n","Requirement already satisfied: attrs>=17.4.0 in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (19.3.0)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from jsonschema>=2.6->nbformat) (57.0.0)\n","Installing collected packages: traitlets, fastjsonschema, nbformat\n","\u001b[33m  WARNING: The script jupyter-trust is installed in '/home/work/.local/bin' which is not on PATH.\n","  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n","  NOTE: The current PATH contains path(s) starting with `~`, which may not be expanded by all applications.\u001b[0m\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","nbclient 0.5.1 requires jupyter-client>=6.1.5, but you have jupyter-client 6.0.0 which is incompatible.\u001b[0m\n","Successfully installed fastjsonschema-2.16.2 nbformat-5.7.1 traitlets-5.8.0\n","\u001b[33mWARNING: You are using pip version 21.1.2; however, version 22.3.1 is available.\n","You should consider upgrading via the '/usr/bin/python3 -m pip install --upgrade pip' command.\u001b[0m\n"]}],"source":["# !pip install nbformat # 설치 시도 \n","!pip install -U nbformat # 기설치시, update"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NtqDJONzX9VU"},"outputs":[],"source":["import os\n","import re\n","import numpy as np\n","from tqdm import tqdm\n","from glob import glob\n","import json\n","import requests\n","import tensorflow as tf\n","from transformers import BertModel, TFBertModel, TFRobertaModel, RobertaTokenizer\n","from transformers import AutoTokenizer, AutoModelForSequenceClassification, AutoModel, AutoModelForSequenceClassification\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split,KFold,StratifiedKFold\n","from adabelief_pytorch import AdaBelief\n","from transformers import AdamW\n","from transformers.optimization import get_cosine_schedule_with_warmup\n","from tqdm import tqdm, tqdm_notebook\n","import shutil\n","import gc\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","\n","from sklearn.metrics import precision_score, recall_score, f1_score, classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j6M_mRDZHmvL"},"outputs":[],"source":["import os\n","os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n","os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""]},{"cell_type":"markdown","metadata":{"id":"tpYsLbtkCx3B"},"source":["\"klue/roberta-large\" 사용 시 \"klue/roberta-large\" 임베딩과 NUM_EPOCHS = 3\n","\n","\"monologg/kobigbird-bert-base\" 모델 사용시  \"monologg/kobigbird-bert-base\" 임베딩으로  NUM_EPOCHS = 6"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GKzmUkXxaNk3"},"outputs":[],"source":["#random seed 고정\n","tf.random.set_seed(1234)\n","np.random.seed(1234)\n","BATCH_SIZE = 64\n","NUM_EPOCHS = 3\n","\n","L_RATE = 1e-5\n","MAX_LEN = 128\n","max_grad_norm=1\n","log_interval=200\n","NUM_CORES = os.cpu_count()\n","device = torch.device(\"cuda:0\")\n","\n","DATA_IN_PATH = './data'\n","DATA_OUT_PATH = \"./data\"\n","SUB_PATH = 'sub'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ngp2zofMaOCp"},"outputs":[],"source":["#두가지 임베딩\n","#\"klue/roberta-large\"\n","#\"monologg/koelectra-base-v3-discriminator\"\n","#tokenizer = AutoTokenizer.from_pretrained(\"monologg/kobigbird-bert-base\", cache_dir='bert_ckpt', do_lower_case=False)\n","tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-large\", cache_dir='bert_ckpt', do_lower_case=False)\n","#tokenizer = AutoTokenizer.from_pretrained(\"monologg/koelectra-base-v3-discriminator\", cache_dir='bert_ckpt', do_lower_case=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"boaPQVIkbMuh"},"outputs":[],"source":["train = pd.read_csv('data/train.csv')\n","test = pd.read_csv('data/test.csv')\n","submission = pd.read_csv('data/sample_submission.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"G--uLszbHmvQ"},"outputs":[],"source":["# 2. Label Encoding (유형, 극성, 시제, 확실성)\n","from sklearn.preprocessing import LabelEncoder\n","type_le = LabelEncoder()\n","train[\"유형\"] = type_le.fit_transform(train[\"유형\"])\n","\n","polarity_le = LabelEncoder()\n","train[\"극성\"] = polarity_le.fit_transform(train[\"극성\"])\n","\n","tense_le = LabelEncoder()\n","train[\"시제\"] = tense_le.fit_transform(train[\"시제\"])\n","\n","certainty_le = LabelEncoder()\n","train[\"확실성\"] = certainty_le.fit_transform(train[\"확실성\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ISVHHL-7rHav"},"outputs":[],"source":["class TrainDataset(Dataset):\n","    def __init__(self, df):\n","        self.df_data = df\n","    def __getitem__(self, index):\n","        # get the sentence from the dataframe\n","        sentence = self.df_data.loc[index, '문장']\n","        encoded_dict = tokenizer(\n","          text = sentence,\n","          add_special_tokens = True, \n","          max_length = MAX_LEN,\n","          pad_to_max_length = True,\n","          truncation=True,           # Pad & truncate all sentences.\n","          return_tensors=\"pt\") #pytorch tensor\n","\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        token_type_id = encoded_dict['token_type_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","        target = torch.tensor(self.df_data.loc[index, \"유형\"])\n","        sample = (padded_token_list, token_type_id , att_mask, target)\n","        return sample\n","    def __len__(self):\n","        return len(self.df_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hsQDbdMKrIyn"},"outputs":[],"source":["class TestDataset(Dataset):\n","    def __init__(self, df):\n","        self.df_data = df\n","    def __getitem__(self, index):\n","        # get the sentence from the dataframe\n","        sentence = self.df_data.loc[index, '문장']\n","        encoded_dict = tokenizer(\n","          text = sentence,\n","          add_special_tokens = True, \n","          max_length = MAX_LEN,\n","          pad_to_max_length = True,\n","          truncation=True,           # Pad & truncate all sentences.\n","          return_tensors=\"pt\")\n","\n","        padded_token_list = encoded_dict['input_ids'][0]\n","        token_type_id = encoded_dict['token_type_ids'][0]\n","        att_mask = encoded_dict['attention_mask'][0]\n","        sample = (padded_token_list, token_type_id , att_mask)\n","        return sample\n","    def __len__(self):\n","        return len(self.df_data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TSFLp8fiq02N"},"outputs":[],"source":["def calc_accuracy(X,Y):\n","    max_vals, max_indices = torch.max(X, 1)\n","    train_acc = (max_indices == Y).sum().data.cpu().numpy()/max_indices.size()[0]\n","    return train_acc"]},{"cell_type":"markdown","metadata":{"id":"paWaosotByFp"},"source":["h값으로 미세조정한 후 저장\n","ex) Roberta229 = h값 229"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7J2-zCLqbdCy","outputId":"76b81575-ff80-4765-cffd-9e47d3cab64b"},"outputs":[{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at klue/roberta-large were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n","- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-large and are newly initialized: ['classifier.dense.weight', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.out_proj.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"ename":"RuntimeError","evalue":"CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1.","output_type":"error","traceback":["\u001b[0;31m\u001b[0m","\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)","\u001b[0;32m<ipython-input-57-198da93b9eec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;31m# print(param)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAdaBelief\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbetas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.9\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0.999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decouple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrectify\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;31m# optimizer = AdamW(model.parameters(),\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    850\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 852\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    853\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m     def register_backward_hook(\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    528\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    550\u001b[0m                 \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 552\u001b[0;31m                     \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    553\u001b[0m                 \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    848\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    849\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 850\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    851\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    852\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call,so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1."]}],"source":["#332 : epoch 3 train acc 0.9187989758050054\n","# 273 : epoch 3 train acc 0.9268683707635386\n","# 325 : epoch 3 train acc 0.9205169095085536\n","# 251 : epoch 3 train acc 0.9313042428174262\n","# kobig120 : epoch 3 train acc 0.9093886905876433\n","\n","#극성\n","#332 : epoch 3 train acc 0.9854487346754297\n","#273 : epoch 3 train acc 0.9868469425354284\n","#325 : epoch 3 train acc 0.984686812931019\n","#251 : epoch 3 train acc 0.988702150120287\n","# kobig120 : epoch 3 train acc 0.9812677962160321\n","\n","#시제\n","#332 : epoch 3 train acc 0.919275517196581\n","#273 : epoch 3 train acc 0.9312424963644657\n","#325 : epoch 3 train acc 0.9180134893123439\n","#251 : epoch 3 train acc 0.9349046320802006\n","\n","#확실성\n","# 332 : epoch 3 train acc 0.9496036063037623\n","# 273 : epoch 3 train acc 0.9542198576228881\n","# 325 : epoch 3 train acc 0.9492931109324948\n","# 251 : epoch 3 train acc 0.9584442313345445\n","#AUto \n","train_data = TrainDataset(train)\n","test_data = TestDataset(test)\n","train_dataloader = torch.utils.data.DataLoader(train_data,\n","                                        batch_size=BATCH_SIZE,\n","                                        shuffle=True,\n","                                      num_workers=NUM_CORES)\n","test_dataloader = torch.utils.data.DataLoader(test_data,\n","                                        batch_size=BATCH_SIZE,\n","                                        shuffle=False,\n","                                      num_workers=NUM_CORES)\n","#두가지 모델\n","#\"klue/roberta-large\"\n","#\"monologg/koelectra-base-v3-discriminator\"\n","#kobigbird-bert-base\n","#model = AutoModelForSequenceClassification.from_pretrained(\"skt/kogpt2-base-v2\",um_labels=3)\n","model = AutoModelForSequenceClassification.from_pretrained(\"klue/roberta-large\", num_labels=4)\n","\n","####미세조정\n","n=0\n","for name, child in model.named_children():\n","    if n==0:\n","      h=0\n","      for param in child.parameters():\n","        if h<=273: #이부분 숫자 조절로 fine-tuning => Roberta229: h=229\n","                   # \n","          param.requires_grad = False\n","        h+=1\n","    n+=1\n","\n","# print(param)\n","model.to(device)\n","optimizer = AdaBelief(model.parameters(), lr=1e-5, eps=1e-16, betas=(0.9,0.999), weight_decouple = True, rectify = False)\n","# optimizer = AdamW(model.parameters(),\n","# lr = 1e-5, # 학습률\n","# eps = 1e-8 # 0으로 나누는 것을 방지하기 위한 epsilon 값,\n","# )\n","warmup_ratio = 0.1\n","t_total = len(train_dataloader) * NUM_EPOCHS\n","warmup_step = int(t_total * warmup_ratio)\n","scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_step, num_training_steps=t_total)\n","\n","for e in range(NUM_EPOCHS):\n","    train_acc = 0.0\n","    test_acc = 0.0\n","    best_acc =0.0\n","    model.train()\n","    torch.set_grad_enabled(True)\n","    for batch_id, (input_id, token_type_id, attention_mask, label) in enumerate(tqdm_notebook(train_dataloader)):\n","        optimizer.zero_grad()\n","        input_id = input_id.long().to(device)\n","        token_type_id = token_type_id.long().to(device)\n","        attention_mask = attention_mask.long().to(device)\n","        label = label.to(device)\n","        outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask, labels=label)\n","        loss = outputs[0]\n","        out = outputs[1]\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n","        optimizer.step()\n","        scheduler.step()  # Update learning rate schedule\n","        _, preds = torch.max(out, dim=1)\n","        train_acc += f1_score(preds.cpu(), label.cpu(), average='weighted')\n","        if batch_id % log_interval == 0:\n","            print(\"epoch {} batch id {} loss {} train acc {}\".format(e+1, batch_id+1, loss.data.cpu().numpy(), train_acc / (batch_id+1)))\n","    print(\"epoch {} train acc {}\".format(e+1, train_acc / (batch_id+1)))\n","\n","preds = [] \n","model.eval()\n","torch.set_grad_enabled(False)\n","for batch_id, (input_id,token_type_id,attention_mask) in enumerate(tqdm_notebook(test_dataloader)):\n","    input_id = input_id.long().to(device)\n","    token_type_id = token_type_id.long().to(device)\n","    attention_mask = attention_mask.long().to(device)\n","    outputs = model(input_ids=input_id, token_type_ids=token_type_id, attention_mask=attention_mask)\n","    out = outputs[0]\n","    for inp in out:\n","      preds.append(inp.detach().cpu().numpy())\n","Preds = np.array(preds)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sb_rZ2NlHmvT"},"outputs":[],"source":["# !ps aux | grep python"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rQBKSREUHmvT"},"outputs":[],"source":["# !kill -9 "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U97xOzbcHmvT"},"outputs":[],"source":["results = np.argmax(Preds, axis=1)\n","#submission['topic_idx']= results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n5QblRSiHmvU"},"outputs":[],"source":["#test_pred=type_le.inverse_transform(results)\n","polarity_pred=polarity_le.inverse_transform(results)\n","#tense_pred=tense_le.inverse_transform(results)\n","#certainty_le_pred=certainty_le.inverse_transform(results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ePitLwgBHmvU"},"outputs":[],"source":["submission['유형']= test_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fv2mEMJcHmvU","outputId":"2923e8cd-f062-4027-e3b3-903971645b05"},"outputs":[{"data":{"text/plain":["array(['긍정', '긍정', '긍정', ..., '긍정', '긍정', '긍정'], dtype=object)"]},"execution_count":99,"metadata":{},"output_type":"execute_result"}],"source":["polarity_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OQ3Xvk9qHmvU","outputId":"19bfd68e-7885-4cde-b728-0fe46dc6e67d"},"outputs":[{"data":{"text/plain":["사실형    7090\n","Name: 유형, dtype: int64"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["submission.유형.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tsoC33CfHmvU"},"outputs":[],"source":["submission['극성']= polarity_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"z3jVhREcHmvV","outputId":"2801c913-fd4a-4ebf-fe34-04e155b54f8d"},"outputs":[{"data":{"text/plain":["긍정    6754\n","부정     287\n","미정      49\n","Name: 극성, dtype: int64"]},"execution_count":101,"metadata":{},"output_type":"execute_result"}],"source":["submission.극성.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F7XrewRGHmvV"},"outputs":[],"source":["submission['시제']= tense_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9y1rOiAjHmvV","outputId":"ef2edf3e-970e-489c-f07e-f89171096d1e"},"outputs":[{"data":{"text/plain":["과거    3462\n","현재    2961\n","미래     667\n","Name: 시제, dtype: int64"]},"execution_count":79,"metadata":{},"output_type":"execute_result"}],"source":["submission.시제.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MbbEfizAHmvV"},"outputs":[],"source":["submission['확실성']= certainty_le_pred"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ibMj1t7lHmvW","outputId":"46c44455-3184-49bf-a46d-2e0dbc462bc4"},"outputs":[{"data":{"text/plain":["확실     6627\n","불확실     463\n","Name: 확실성, dtype: int64"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["submission.확실성.value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"omhypQx3HmvW"},"outputs":[],"source":["submission['label']=submission.유형+'-'+submission.극성+'-'+submission.시제+'-'+submission.확실성"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g-ue8YQ0HmvW"},"outputs":[],"source":["last=submission[['ID','label']]\n","last=last.set_index('ID')\n","last.to_csv('klue_최종label.csv',encoding='utf-8-sig',)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dnVtr49tHmvW"},"outputs":[],"source":["Roberta251_certainty=Preds\n","np.save(SUB_PATH+'Roberta251_certainty.npy',arr=Roberta251_certainty)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q45Om9fxHF33"},"outputs":[],"source":["kobig120=Preds \n","np.save(SUB_PATH+'Electra120.npy',arr=kobig120)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFM-3HIYHmvW"},"outputs":[],"source":["#332 : epoch 3 train acc 0.9187989758050054\n","# 273 : epoch 3 train acc 0.9268683707635386\n","# 325 : epoch 3 train acc 0.9205169095085536\n","# 251 : epoch 3 train acc 0.9313042428174262\n","# kobig120 : epoch 3 train acc 0.9093886905876433\n","\n","#극성\n","#332 : epoch 3 train acc 0.9854487346754297\n","#273 : epoch 3 train acc 0.9868469425354284\n","#325 : epoch 3 train acc 0.984686812931019\n","#251 : epoch 3 train acc 0.988702150120287\n","# kobig120 : epoch 3 train acc 0.9812677962160321\n","#시제\n","#332 : epoch 3 train acc 0.919275517196581\n","#273 : epoch 3 train acc 0.9312424963644657\n","#325 : epoch 3 train acc 0.9180134893123439\n","#251 : epoch 3 train acc 0.9349046320802006\n","#\n","\n","#확실성\n","# 332 : epoch 3 train acc 0.9496036063037623\n","# 273 : epoch 3 train acc 0.9539584020942307\n","# 325 : epoch 3 train acc 0.9492931109324948\n","# 251 : epoch 3 train acc 0.9584442313345445"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ol5c7wf3HmvX"},"outputs":[],"source":["kobig120=np.load('numpy값Electra120.npy')\n","Roberta251=np.load('numpy값Roberta251.npy')\n","Roberta273=np.load('numpy값Roberta273.npy')\n","Roberta325=np.load('numpy값Roberta325.npy')\n","Roberta332=np.load('numpy값Roberta332_type.npy')\n","\n","Roberta332_polarity= np.load('numpy값Roberta332_polarity.npy')\n","Roberta325_polarity= np.load('numpy값Roberta325_polarity.npy')\n","Roberta273_polarity= np.load('numpy값Roberta273_polarity.npy')\n","Roberta251_polarity= np.load('numpy값Roberta251_polarirty.npy')\n","kobig120_polarity=np.load('numpy값kobig120_polarity.npy')\n","\n","Roberta332_tense= np.load('numpy값Roberta332_tense.npy')\n","Roberta325_tense= np.load('numpy값Roberta325_tense.npy')\n","Roberta273_tense= np.load('numpy값Roberta273_tense.npy')\n","Roberta251_tense= np.load('numpy값Roberta251_tense.npy')\n","kobig120_tense=np.load('numpy값kobig120_tense.npy')\n","\n","Roberta332_certainty= np.load('numpy값Roberta332_certainty.npy')\n","Roberta325_certainty= np.load('numpy값Roberta325_certainty.npy')\n","Roberta273_certainty= np.load('numpy값Roberta273_certainty.npy')\n","Roberta251_certainty= np.load('numpy값Roberta251_certainty.npy')\n","kobig120_certainty=np.load('numpy값kobig120_certainty.npy')\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5ICkbL9SG9qE"},"outputs":[],"source":["Pred_values_type = Roberta332*0.1 + Roberta325 * 0.15 + Roberta273 * 0.15 + Roberta251 * 0.4 + kobig120 * 0.2\n","Pred_values_polarity = Roberta332_polarity*0.1 + Roberta325_polarity * 0.15 + Roberta273_polarity * 0.15 + Roberta251_polarity * 0.4 + kobig120_polarity * 0.2\n","Pred_values_tense = Roberta332_tense*0.1 + Roberta325_tense * 0.15 + Roberta273_tense * 0.15 + Roberta251_tense * 0.4 + kobig120_tense * 0.2\n","Pred_values_certainty = Roberta332_certainty*0.1 + Roberta325_certainty * 0.15 + Roberta273_certainty * 0.15 + Roberta251_certainty * 0.4 + kobig120_certainty * 0.2"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kwHLSFtYxOT_"},"outputs":[],"source":["results_type = np.argmax(Pred_values_type, axis=1)\n","type_pred_ang=type_le.inverse_transform(results_type)\n","submission['유형']= type_pred_ang\n","\n","results_polarity = np.argmax(Pred_values_polarity, axis=1)\n","polarity_pred_ang=polarity_le.inverse_transform(results_polarity)\n","submission['극성']= polarity_pred_ang\n","\n","results_tense = np.argmax(Pred_values_tense, axis=1)\n","tense_pred_ang=tense_le.inverse_transform(results_tense)\n","submission['시제']= tense_pred_ang\n","\n","results_certainty = np.argmax(Pred_values_certainty, axis=1)\n","certainty_pred_ang=certainty_le.inverse_transform(results_certainty)\n","submission['확실성']= certainty_pred_ang\n","\n","submission['label']=submission.유형+'-'+submission.극성+'-'+submission.시제+'-'+submission.확실성\n","last=submission[['ID','label']]\n","last=last.set_index('ID')\n","last.to_csv('klue_kobig_ang_최종label.csv',encoding='utf-8-sig',)\n","#submission.to_csv(PATH + 'daconnews.csv', index=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ad0n7954HmvX","outputId":"0d8612c8-aabf-4ef1-ff8c-db67584cbddd"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>ID</th>\n","      <th>label</th>\n","      <th>유형</th>\n","      <th>극성</th>\n","      <th>시제</th>\n","      <th>확실성</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>TEST_0000</td>\n","      <td>사실형-긍정-현재-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>현재</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>TEST_0001</td>\n","      <td>사실형-긍정-현재-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>현재</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>TEST_0002</td>\n","      <td>사실형-긍정-과거-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>과거</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>TEST_0003</td>\n","      <td>사실형-긍정-현재-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>현재</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>TEST_0004</td>\n","      <td>사실형-긍정-과거-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>과거</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>7085</th>\n","      <td>TEST_7085</td>\n","      <td>사실형-긍정-현재-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>현재</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>7086</th>\n","      <td>TEST_7086</td>\n","      <td>추론형-긍정-현재-확실</td>\n","      <td>추론형</td>\n","      <td>긍정</td>\n","      <td>현재</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>7087</th>\n","      <td>TEST_7087</td>\n","      <td>사실형-긍정-현재-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>현재</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>7088</th>\n","      <td>TEST_7088</td>\n","      <td>추론형-긍정-미래-확실</td>\n","      <td>추론형</td>\n","      <td>긍정</td>\n","      <td>미래</td>\n","      <td>확실</td>\n","    </tr>\n","    <tr>\n","      <th>7089</th>\n","      <td>TEST_7089</td>\n","      <td>사실형-긍정-과거-확실</td>\n","      <td>사실형</td>\n","      <td>긍정</td>\n","      <td>과거</td>\n","      <td>확실</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>7090 rows × 6 columns</p>\n","</div>"],"text/plain":["             ID         label   유형  극성  시제 확실성\n","0     TEST_0000  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n","1     TEST_0001  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n","2     TEST_0002  사실형-긍정-과거-확실  사실형  긍정  과거  확실\n","3     TEST_0003  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n","4     TEST_0004  사실형-긍정-과거-확실  사실형  긍정  과거  확실\n","...         ...           ...  ...  ..  ..  ..\n","7085  TEST_7085  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n","7086  TEST_7086  추론형-긍정-현재-확실  추론형  긍정  현재  확실\n","7087  TEST_7087  사실형-긍정-현재-확실  사실형  긍정  현재  확실\n","7088  TEST_7088  추론형-긍정-미래-확실  추론형  긍정  미래  확실\n","7089  TEST_7089  사실형-긍정-과거-확실  사실형  긍정  과거  확실\n","\n","[7090 rows x 6 columns]"]},"execution_count":25,"metadata":{},"output_type":"execute_result"}],"source":["submission"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1mOuKGiAHmvX"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"PyTorch 1.9.0 on Python 3.8 (CUDA 11.1)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.7"}},"nbformat":4,"nbformat_minor":0}